From 239e200a27cf7cbf62bd72228bd769fe11b662a9 Mon Sep 17 00:00:00 2001
From: Pratyush Yadav <p.yadav@ti.com>
Date: Fri, 9 Jul 2021 22:35:33 +0530
Subject: [tiL5.10-CON PATCH 1/2] media: ti: j721e-csi2rx: Drain DMA when
 dropping frames

Once frames start dropping due to lack of buffers, some data gets stuck
in the DMA pipeline somewhere. So the first DMA transfer after frame
drops gives a partial frame. This is obviously not useful to the
application and will only serve to confuse it. Issue a DMA transaction
to drain that up so that the next frame comes out clean.

Signed-off-by: Pratyush Yadav <p.yadav@ti.com>
---
 .../platform/ti/j721e-csi2rx/j721e-csi2rx.c   | 106 ++++++++++++++++--
 1 file changed, 97 insertions(+), 9 deletions(-)

diff --git a/drivers/media/platform/ti/j721e-csi2rx/j721e-csi2rx.c b/drivers/media/platform/ti/j721e-csi2rx/j721e-csi2rx.c
index 524f8780f064..62251ac51cdf 100644
--- a/drivers/media/platform/ti/j721e-csi2rx/j721e-csi2rx.c
+++ b/drivers/media/platform/ti/j721e-csi2rx/j721e-csi2rx.c
@@ -44,6 +44,8 @@
 
 #define PSIL_WORD_SIZE_BYTES		16
 
+#define DRAIN_TIMEOUT_MS		50
+
 struct ti_csi2rx_fmt {
 	u32				fourcc;	/* Four character code. */
 	u32				code;	/* Mbus code. */
@@ -485,6 +487,59 @@ static void ti_csi2rx_setup_shim(struct ti_csi2rx_dev *csi)
 	writel(reg, csi->shim + SHIM_PSI_CFG0);
 }
 
+static void ti_csi2rx_drain_callback(void *param)
+{
+	struct completion *drain_complete = param;
+
+	complete(drain_complete);
+}
+
+static int ti_csi2rx_drain_dma(struct ti_csi2rx_dev *csi)
+{
+	struct dma_async_tx_descriptor *desc;
+	struct device *dev = csi->dma.chan->device->dev;
+	struct completion drain_complete;
+	void *buf;
+	size_t len = csi->v_fmt.fmt.pix.sizeimage;
+	dma_addr_t addr;
+	dma_cookie_t cookie;
+	int ret;
+
+	init_completion(&drain_complete);
+
+	buf = dma_alloc_coherent(dev, len, &addr, GFP_KERNEL | GFP_ATOMIC);
+	if (!buf)
+		return -ENOMEM;
+
+	desc = dmaengine_prep_slave_single(csi->dma.chan, addr, len,
+					   DMA_DEV_TO_MEM,
+					   DMA_PREP_INTERRUPT | DMA_CTRL_ACK);
+	if (!desc) {
+		ret = -EIO;
+		goto out;
+	}
+
+	desc->callback = ti_csi2rx_drain_callback;
+	desc->callback_param = &drain_complete;
+
+	cookie = dmaengine_submit(desc);
+	ret = dma_submit_error(cookie);
+	if (ret)
+		goto out;
+
+	dma_async_issue_pending(csi->dma.chan);
+
+	if (!wait_for_completion_timeout(&drain_complete,
+					 msecs_to_jiffies(DRAIN_TIMEOUT_MS))) {
+		dmaengine_terminate_sync(csi->dma.chan);
+		ret = -ETIMEDOUT;
+		goto out;
+	}
+out:
+	dma_free_coherent(dev, len, buf, addr);
+	return ret;
+}
+
 static void ti_csi2rx_dma_callback(void *param)
 {
 	struct ti_csi2rx_buffer *buf = param;
@@ -588,6 +643,7 @@ static void ti_csi2rx_buffer_queue(struct vb2_buffer *vb)
 	struct ti_csi2rx_dev *csi = vb2_get_drv_priv(vb->vb2_queue);
 	struct ti_csi2rx_buffer *buf;
 	struct ti_csi2rx_dma *dma = &csi->dma;
+	bool restart_dma = false;
 	unsigned long flags = 0;
 	int ret;
 
@@ -600,21 +656,43 @@ static void ti_csi2rx_buffer_queue(struct vb2_buffer *vb)
 	 * But if DMA has stalled due to lack of buffers, restart it now.
 	 */
 	if (dma->state == TI_CSI2RX_DMA_IDLE) {
-		ret = ti_csi2rx_start_dma(csi, buf);
-		if (ret) {
-			dev_err(csi->dev, "Failed to start DMA: %d\n", ret);
-			vb2_buffer_done(&buf->vb.vb2_buf, VB2_BUF_STATE_QUEUED);
-			goto unlock;
-		}
-
+		/*
+		 * Do not restart DMA with the lock held because
+		 * ti_csi2rx_drain_dma() might block when allocating a buffer.
+		 * There won't be a race on queueing DMA anyway since the
+		 * callback is not being fired.
+		 */
+		restart_dma = true;
 		dma->curr = buf;
 		dma->state = TI_CSI2RX_DMA_ACTIVE;
 	} else {
 		list_add_tail(&buf->list, &dma->queue);
 	}
-
-unlock:
 	spin_unlock_irqrestore(&dma->lock, flags);
+
+	if (restart_dma) {
+		/*
+		 * Once frames start dropping, some data gets stuck in the DMA
+		 * pipeline somewhere. So the first DMA transfer after frame
+		 * drops gives a partial frame. This is obviously not useful to
+		 * the application and will only confuse it. Issue a DMA
+		 * transaction to drain that up.
+		 */
+		ret = ti_csi2rx_drain_dma(csi);
+		if (ret)
+			dev_warn(csi->dev,
+				 "Failed to drain DMA. Next frame might be bogus\n");
+
+		ret = ti_csi2rx_start_dma(csi, buf);
+		if (ret) {
+			dev_err(csi->dev, "Failed to start DMA: %d\n", ret);
+			spin_lock_irqsave(&dma->lock, flags);
+			vb2_buffer_done(&buf->vb.vb2_buf, VB2_BUF_STATE_ERROR);
+			dma->curr = NULL;
+			dma->state = TI_CSI2RX_DMA_IDLE;
+			spin_unlock_irqrestore(&dma->lock, flags);
+		}
+	}
 }
 
 /*
@@ -764,6 +842,7 @@ static void ti_csi2rx_stop_streaming(struct vb2_queue *vq)
 	struct ti_csi2rx_buffer *buf = NULL, *tmp;
 	struct ti_csi2rx_dma *dma = &csi->dma;
 	unsigned long flags = 0;
+	enum ti_csi2rx_dma_state state;
 	int ret;
 
 	media_pipeline_stop(csi->vdev.entity.pads);
@@ -789,9 +868,18 @@ static void ti_csi2rx_stop_streaming(struct vb2_queue *vq)
 	if (dma->curr)
 		vb2_buffer_done(&dma->curr->vb.vb2_buf, VB2_BUF_STATE_ERROR);
 
+	state = dma->state;
+
 	dma->curr = NULL;
 	dma->state = TI_CSI2RX_DMA_STOPPED;
 	spin_unlock_irqrestore(&dma->lock, flags);
+
+	if (state == TI_CSI2RX_DMA_IDLE) {
+		ret = ti_csi2rx_drain_dma(csi);
+		if (ret)
+			dev_warn(csi->dev,
+				 "Failed to drain DMA. Next frame might be bogus\n");
+	}
 }
 
 static const struct vb2_ops csi_vb2_qops = {
-- 
2.30.0

