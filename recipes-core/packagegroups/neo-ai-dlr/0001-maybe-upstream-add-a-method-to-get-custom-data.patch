From b501a6225a88ad055d0526fafb3ead502d865694 Mon Sep 17 00:00:00 2001
From: Subhajit Paul <subhajit_paul@ti.com>
Date: Fri, 11 Sep 2020 21:27:37 +0530
Subject: [PATCH] add a method to get custom data

---
 include/dlr.h           |  4 ++
 include/dlr_common.h    | 10 +++++
 include/dlr_tvm.h       |  1 +
 python/dlr/api.py       | 11 +++++
 python/dlr/dlr_model.py | 18 +++++++-
 src/dlr.cc              | 91 ++++++++++++++++++++++++++++++++++++++++-
 src/dlr_tvm.cc          | 13 ++++++
 7 files changed, 146 insertions(+), 2 deletions(-)

diff --git a/include/dlr.h b/include/dlr.h
index 9110933..198479d 100644
--- a/include/dlr.h
+++ b/include/dlr.h
@@ -430,6 +430,10 @@ int SetDLRNumThreads(DLRModelHandle* handle, int threads);
 DLR_DLL
 int UseDLRCPUAffinity(DLRModelHandle* handle, int use);
 
+DLR_DLL
+int GetDLRTIBenchmarkData(DLRModelHandle* handle, const char ***annotations,
+		uint64_t **vals, int *count);
+
 /*!
  * \brief Set custom allocator malloc function. Must be called before CreateDLRModel or
  *        CreateDLRPipeline. It is recommended to use with SetDLRCustomAllocatorFree and
diff --git a/include/dlr_common.h b/include/dlr_common.h
index 6da510d..e4f2f4a 100644
--- a/include/dlr_common.h
+++ b/include/dlr_common.h
@@ -148,6 +148,10 @@ class DLR_DLL DLRModel {
   virtual const std::vector<int64_t>& GetInputShape(int index) const;
   virtual void GetInput(const char* name, void* input) = 0;
   virtual void SetInput(const char* name, const int64_t* shape, const void* input, int dim) = 0;
+  virtual bool GetCustomData(const char* name, void **out) {
+    LOG(ERROR) << "GetCustomData is not supported yet!";
+    return false;
+  };
 
   /* Output related functions */
   virtual int GetNumOutputs() { return num_outputs_; }
@@ -179,6 +183,12 @@ class DLR_DLL DLRModel {
   virtual bool HasMetadata() const;
   virtual void UseCPUAffinity(bool use) = 0;
   virtual void Run() = 0;
+
+  uint64_t run_start_ts, run_start_ddr_read, run_start_ddr_write;
+  uint64_t run_end_ts, run_end_ddr_read, run_end_ddr_write;
+  std::vector<std::pair<std::string, uint64_t>> benchmarks;
+  std::unique_ptr<const char *[]> annotations;
+  std::unique_ptr<uint64_t[]> vals;
 };
 
 typedef std::shared_ptr<DLRModel> DLRModelPtr;
diff --git a/include/dlr_tvm.h b/include/dlr_tvm.h
index e5c4f07..c38d3c8 100644
--- a/include/dlr_tvm.h
+++ b/include/dlr_tvm.h
@@ -47,6 +47,7 @@ class DLR_DLL TVMModel : public DLRModel {
   virtual void GetInput(const char* name, void* input) override;
   virtual void SetInput(const char* name, const int64_t* shape, const void* input,
                         int dim) override;
+  virtual bool GetCustomData(const char* name, void **out) override;
   void SetInputTensor(const char* name, DLTensor* tensor);
   void SetInputTensorZeroCopy(const char* name, DLTensor* tensor);
 
diff --git a/python/dlr/api.py b/python/dlr/api.py
index d3c8421..489b74f 100644
--- a/python/dlr/api.py
+++ b/python/dlr/api.py
@@ -32,6 +32,10 @@ class IDLRModel:
     def run(self, input_data):
         raise NotImplementedError
 
+    @abc.abstractmethod
+    def get_TI_benchmark_data(self):
+        raise NotImplementedError
+
 
 def _find_model_file(model_path, ext):
     if os.path.isfile(model_path) and model_path.endswith(ext):
@@ -292,3 +296,10 @@ class DLRModel(IDLRModel):
         except Exception as ex:
             self.neo_logger.exception("error in getting output data type {} {}".format(self._impl.__class__.__name__, ex))
             raise ex
+
+    def get_TI_benchmark_data(self):
+        try:
+            return self._impl.get_TI_benchmark_data()
+        except Exception as ex:
+            self.neo_logger.exception("error in getting TI benchmark data \"{}\" {} {}".format(self._impl.__class__.__name__, ex))
+            raise ex
diff --git a/python/dlr/dlr_model.py b/python/dlr/dlr_model.py
index 9360538..e45ded9 100755
--- a/python/dlr/dlr_model.py
+++ b/python/dlr/dlr_model.py
@@ -1,6 +1,6 @@
 # coding: utf-8
 import ctypes
-from ctypes import c_void_p, c_int, c_char_p, byref, POINTER, c_longlong
+from ctypes import c_void_p, c_int, c_char_p, byref, POINTER, c_longlong, c_uint64
 import json
 import numpy as np
 import os
@@ -481,3 +481,19 @@ class DLRModelImpl(IDLRModel):
                                      out.ctypes._as_parameter_))
         out = out.reshape(shape)
         return out
+
+    def get_TI_benchmark_data(self):
+        count = c_int(0)
+        annotations = POINTER(c_char_p)()
+        vals = POINTER(c_uint64)()
+        self._check_call(self._lib.GetDLRTIBenchmarkData(byref(self.handle),
+                                        byref(annotations),
+                                        byref(vals),
+                                        byref(count)))
+        out = {}
+        for i in range(count.value):
+            key = annotations[i].decode()
+            assert key not in out.keys()
+            out[key] = vals[i]
+
+        return out
diff --git a/src/dlr.cc b/src/dlr.cc
index 9ea1853..3705930 100644
--- a/src/dlr.cc
+++ b/src/dlr.cc
@@ -14,6 +14,13 @@
 
 using namespace dlr;
 
+static inline void get_time_u64(uint64_t *t)
+{
+    struct timespec ts;
+    clock_gettime(CLOCK_MONOTONIC, &ts);
+    *t = (uint64_t)ts.tv_sec * (uint64_t)1000000000ull + (uint64_t)ts.tv_nsec;
+}
+
 /* DLR C API implementation */
 
 extern "C" int GetDLRNumInputs(DLRModelHandle* handle, int* num_inputs) {
@@ -401,9 +408,34 @@ extern "C" int DeleteDLRModel(DLRModelHandle* handle) {
   API_END();
 }
 
+static inline void get_ddr_stats(DLRModel *model, uint64_t *read, uint64_t *write) {
+  void *out;
+  bool res = model->GetCustomData("tidl_get_custom_data_ddrstats", &out);
+  if(!res) {
+    *read = *write = 0;
+    return;
+  }
+  std::pair<uint64_t, uint64_t> *s = static_cast<std::pair<uint64_t, uint64_t>*>(out);
+  *read = s->first;
+  *write = s->second;
+  delete s;
+}
+
+
 extern "C" int RunDLRModel(DLRModelHandle* handle) {
   API_BEGIN();
-  static_cast<DLRModel*>(*handle)->Run();
+  DLRModel* model = static_cast<DLRModel*>(*handle);
+  get_ddr_stats(model, &model->run_start_ddr_read, &model->run_start_ddr_write);
+  get_time_u64(&model->run_start_ts);
+  model->Run();
+  get_time_u64(&model->run_end_ts);
+  get_ddr_stats(model, &model->run_end_ddr_read, &model->run_end_ddr_write);
+
+  /* adjust for wrap around */
+  if(model->run_end_ddr_read < model->run_start_ddr_read)
+    model->run_end_ddr_read = (uint64_t)0xffffffffffffffffull + model->run_end_ddr_read;
+  if(model->run_end_ddr_write < model->run_start_ddr_write)
+    model->run_end_ddr_write = 0xffffffffffffffffull + model->run_end_ddr_write;
   API_END();
 }
 
@@ -452,6 +484,63 @@ extern "C" int UseDLRCPUAffinity(DLRModelHandle* handle, int use) {
   API_END();
 }
 
+extern "C" int GetDLRTIBenchmarkData(DLRModelHandle* handle, const char ***annotations,
+		uint64_t **vals, int *count) {
+  API_BEGIN();
+  DLRModel* model = static_cast<DLRModel*>(*handle);
+  CHECK(model != nullptr) << "model is nullptr, create it first";
+
+  /* clear out the past data */
+  model->benchmarks = std::vector<std::pair<std::string, uint64_t>>();
+
+  /* get the run duration */
+  model->benchmarks.push_back(std::make_pair<std::string, uint64_t>("ts:run_start", uint64_t(model->run_start_ts)));
+  model->benchmarks.push_back(std::make_pair<std::string, uint64_t>("ts:run_end", uint64_t(model->run_end_ts)));
+
+  /* get the ddr bw numbers */
+  model->benchmarks.push_back(std::make_pair<std::string, uint64_t>("ddr:read_start", uint64_t(model->run_start_ddr_read)));
+  model->benchmarks.push_back(std::make_pair<std::string, uint64_t>("ddr:read_end", uint64_t(model->run_end_ddr_read)));
+  model->benchmarks.push_back(std::make_pair<std::string, uint64_t>("ddr:write_start", uint64_t(model->run_start_ddr_write)));
+  model->benchmarks.push_back(std::make_pair<std::string, uint64_t>("ddr:write_end", uint64_t(model->run_end_ddr_write)));
+
+  /* get the timestamps for subgraphs */
+  void *out;
+  int subgraph_id = 0;
+  while(true) {
+      bool res = model->GetCustomData(("tidl_get_custom_data_" + std::to_string(subgraph_id)).c_str(), &out);
+      if(!res)
+          break;
+      std::vector<uint64_t> *v = static_cast<std::vector<uint64_t>*>(out);
+      std::string annots[] = {
+          "copy_in_start", "copy_in_end",
+          "proc_start", "proc_end",
+          "copy_out_start", "copy_out_end"
+      };
+      int index = 0;
+      for(auto it = v->begin(); it != v->end(); it++, index++)
+          model->benchmarks.push_back(std::make_pair<std::string, uint64_t>(
+                      "ts:subgraph_" + std::to_string(subgraph_id) + "_" + annots[index],
+                      uint64_t(*it)));
+      delete v;
+      subgraph_id++;
+  }
+
+  /* make unique pointers for returning and populate */
+  uint64_t *vals_ptr = (model->vals = std::make_unique<uint64_t[]>(model->benchmarks.size())).get();
+  const char **annotations_ptr = (model->annotations = std::make_unique<const char *[]>(model->benchmarks.size())).get();
+  for(auto it = model->benchmarks.begin(); it != model->benchmarks.end(); it++) {
+      *vals_ptr = (*it).second; vals_ptr++;
+      *annotations_ptr = (*it).first.c_str(); annotations_ptr++;
+  }
+
+  /* return */
+  *annotations = model->annotations.get();
+  *vals = model->vals.get();
+  *count = model->benchmarks.size();
+
+  API_END();
+}
+
 extern "C" int SetDLRCustomAllocatorMalloc(DLRMallocFunctionPtr custom_malloc_fn) {
   API_BEGIN();
   DLRAllocatorFunctions::SetMallocFunction(custom_malloc_fn);
diff --git a/src/dlr_tvm.cc b/src/dlr_tvm.cc
index 09da1d1..029580b 100644
--- a/src/dlr_tvm.cc
+++ b/src/dlr_tvm.cc
@@ -251,6 +251,19 @@ void TVMModel::GetOutputShape(int index, int64_t* shape) const {
   std::memcpy(shape, outputs_[index]->shape, sizeof(int64_t) * outputs_[index]->ndim);
 }
 
+bool TVMModel::GetCustomData(const char *name, void **out)
+{
+  tvm::runtime::PackedFunc get_function = tvm_module_->GetFunction("get_custom_data");
+  CHECK(get_function != nullptr) << "Unsupported \"get_custom_data\"\n";
+
+  auto x = get_function(name, out);
+  if(x.type_code() == kTVMNullptr)
+    return false;
+
+  *out = x;
+  return true;
+}
+
 void TVMModel::GetOutput(int index, void* out) {
   DLTensor output_tensor = *outputs_[index];
   output_tensor.ctx = DLContext{kDLCPU, 0};
-- 
2.17.1

