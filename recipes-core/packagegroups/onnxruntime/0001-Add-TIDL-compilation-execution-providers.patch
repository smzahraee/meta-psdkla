From 1a8f97d3425c77b9aa8c63dc65c2999240eed86d Mon Sep 17 00:00:00 2001
From: Subhajit Paul <subhajit_paul@ti.com>
Date: Thu, 1 Apr 2021 00:52:43 +0530
Subject: [PATCH] Add TIDL compilation / execution providers

Signed-off-by: Subhajit Paul <subhajit_paul@ti.com>
---
 cmake/CMakeLists.txt                          |    5 +-
 cmake/onnxruntime.cmake                       |    1 +
 cmake/onnxruntime_providers.cmake             |   20 +
 cmake/onnxruntime_python.cmake                |    1 +
 cmake/onnxruntime_session.cmake               |    2 +-
 cmake/onnxruntime_unittests.cmake             |    1 +
 .../CXX_Api_Sample.cpp                        |    4 +-
 .../makefile                                  |   30 +
 .../setenv.sh                                 |   12 +
 .../core/framework/execution_provider.h       |    2 +
 include/onnxruntime/core/framework/func_api.h |    4 +
 include/onnxruntime/core/graph/constants.h    |    2 +
 .../providers/dnnl/dnnl_provider_factory.h    |    3 +-
 .../providers/tidl/tidl_provider_factory.h    |   24 +
 onnxruntime/core/framework/func_kernel.h      |   11 +
 .../core/framework/fuse_nodes_funcs.cc        |   11 +
 onnxruntime/core/framework/fuse_nodes_funcs.h |    1 +
 .../core/framework/graph_partitioner.cc       |    4 +
 onnxruntime/core/framework/utils.cc           |    2 +
 .../core/providers/get_execution_providers.cc |   16 +
 onnxruntime/core/providers/tidl/symbols.txt   |    1 +
 .../providers/tidl/tidl_execution_provider.cc |  449 ++++++++
 .../providers/tidl/tidl_execution_provider.h  |   79 ++
 .../tidl/tidl_execution_provider_common.h     |   64 ++
 .../providers/tidl/tidl_provider_factory.cc   |   47 +
 onnxruntime/core/session/inference_session.cc |   96 +-
 onnxruntime/core/session/inference_session.h  |    8 +
 .../onnxruntime_inference_collection.py       |    7 +-
 .../python/onnxruntime_pybind_state.cc        |   46 +-
 onnxruntime/test/util/include/providers.h     |    3 +
 setup.py                                      |    3 +
 tidl_demos/README                             |   17 +
 tidl_demos/basic_demo/CXX_Api_Sample.cpp      |  138 +++
 tidl_demos/basic_demo/C_Api_Sample.cpp        |  170 +++
 tidl_demos/basic_demo/InferenceTestCapi.cpp   |   91 ++
 tidl_demos/basic_demo/README                  |   10 +
 tidl_demos/basic_demo/makefile                |   31 +
 tidl_demos/basic_demo/setenv.sh               |   12 +
 tidl_demos/classification_demo/README         |   25 +
 tidl_demos/classification_demo/main.cc        |  164 +++
 tidl_demos/classification_demo/makefile       |   61 +
 tidl_demos/classification_demo/models.py      |  207 ++++
 tidl_demos/classification_demo/onnxrt_tidl.py |  160 +++
 tidl_demos/classification_demo/setenv.sh      |   17 +
 tidl_demos/classification_demo/validator.cc   |  149 +++
 tidl_demos/classification_demo/validator.h    |   32 +
 tidl_demos/labels/imgnet_labels.txt           | 1000 +++++++++++++++++
 tidl_demos/tidl_build.sh                      |    9 +
 tidl_demos/tools/pics.py                      |   39 +
 tool.cmake                                    |    9 +
 tools/ci_build/build.py                       |    8 +-
 51 files changed, 3292 insertions(+), 16 deletions(-)
 create mode 100644 csharp/test/Microsoft.ML.OnnxRuntime.EndToEndTests.Capi/makefile
 create mode 100755 csharp/test/Microsoft.ML.OnnxRuntime.EndToEndTests.Capi/setenv.sh
 create mode 100644 include/onnxruntime/core/providers/tidl/tidl_provider_factory.h
 create mode 100644 onnxruntime/core/providers/tidl/symbols.txt
 create mode 100644 onnxruntime/core/providers/tidl/tidl_execution_provider.cc
 create mode 100644 onnxruntime/core/providers/tidl/tidl_execution_provider.h
 create mode 100644 onnxruntime/core/providers/tidl/tidl_execution_provider_common.h
 create mode 100644 onnxruntime/core/providers/tidl/tidl_provider_factory.cc
 create mode 100644 tidl_demos/README
 create mode 100644 tidl_demos/basic_demo/CXX_Api_Sample.cpp
 create mode 100644 tidl_demos/basic_demo/C_Api_Sample.cpp
 create mode 100644 tidl_demos/basic_demo/InferenceTestCapi.cpp
 create mode 100644 tidl_demos/basic_demo/README
 create mode 100644 tidl_demos/basic_demo/makefile
 create mode 100755 tidl_demos/basic_demo/setenv.sh
 create mode 100644 tidl_demos/classification_demo/README
 create mode 100644 tidl_demos/classification_demo/main.cc
 create mode 100644 tidl_demos/classification_demo/makefile
 create mode 100644 tidl_demos/classification_demo/models.py
 create mode 100644 tidl_demos/classification_demo/onnxrt_tidl.py
 create mode 100755 tidl_demos/classification_demo/setenv.sh
 create mode 100644 tidl_demos/classification_demo/validator.cc
 create mode 100644 tidl_demos/classification_demo/validator.h
 create mode 100644 tidl_demos/labels/imgnet_labels.txt
 create mode 100755 tidl_demos/tidl_build.sh
 create mode 100644 tidl_demos/tools/pics.py
 create mode 100644 tool.cmake

diff --git a/cmake/CMakeLists.txt b/cmake/CMakeLists.txt
index b9d63a163..fb431fc02 100644
--- a/cmake/CMakeLists.txt
+++ b/cmake/CMakeLists.txt
@@ -35,7 +35,6 @@ include(CheckLanguage)
 # On the other hand it does not allow some others.
 # So we cant' regulate simply with the standard.
 set(CMAKE_CXX_STANDARD 14)
-
 set_property(GLOBAL PROPERTY USE_FOLDERS ON)
 # NOTE: POSITION INDEPENDENT CODE hurts performance, and it only make sense on POSIX systems
 set(CMAKE_POSITION_INDEPENDENT_CODE ON)
@@ -1050,8 +1049,8 @@ else()
   string(APPEND CMAKE_C_FLAGS " -Wall -Wextra")
 
   if(onnxruntime_DEV_MODE)
-    string(APPEND CMAKE_CXX_FLAGS " -Werror")
-    string(APPEND CMAKE_C_FLAGS " -Werror")
+    string(APPEND CMAKE_CXX_FLAGS " -Wno-error")
+    string(APPEND CMAKE_C_FLAGS " -Wno-error")
   endif()
   check_cxx_compiler_flag(-Wunused-but-set-variable HAS_UNUSED_BUT_SET_VARIABLE)
   check_cxx_compiler_flag(-Wunused-parameter HAS_UNUSED_PARAMETER)
diff --git a/cmake/onnxruntime.cmake b/cmake/onnxruntime.cmake
index 3b283767c..f7deaddce 100644
--- a/cmake/onnxruntime.cmake
+++ b/cmake/onnxruntime.cmake
@@ -105,6 +105,7 @@ target_link_libraries(onnxruntime PRIVATE
     ${PROVIDERS_NUPHAR}
     ${PROVIDERS_VITISAI}
     ${PROVIDERS_DML}
+    ${PROVIDERS_TIDL}
     ${PROVIDERS_ACL}
     ${PROVIDERS_ARMNN}
     ${PROVIDERS_INTERNAL_TESTING}
diff --git a/cmake/onnxruntime_providers.cmake b/cmake/onnxruntime_providers.cmake
index a4ea17d7b..700e1b54a 100644
--- a/cmake/onnxruntime_providers.cmake
+++ b/cmake/onnxruntime_providers.cmake
@@ -100,6 +100,10 @@ if(onnxruntime_USE_WINML)
   set(PROVIDERS_WINML onnxruntime_providers_winml)
   list(APPEND ONNXRUNTIME_PROVIDER_NAMES winml)
 endif()
+if(onnxruntime_USE_TIDL)
+  set(PROVIDERS_TIDL onnxruntime_providers_tidl)
+  list(APPEND ONNXRUNTIME_PROVIDER_NAMES tidl)
+endif()
 if(onnxruntime_USE_ACL)
   set(PROVIDERS_ACL onnxruntime_providers_acl)
   list(APPEND ONNXRUNTIME_PROVIDER_NAMES acl)
@@ -878,6 +882,22 @@ if (onnxruntime_USE_MIGRAPHX)
   install(DIRECTORY ${PROJECT_SOURCE_DIR}/../include/onnxruntime/core/providers/migraphx  DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}/onnxruntime/core/providers)
   set_target_properties(onnxruntime_providers_migraphx PROPERTIES LINKER_LANGUAGE CXX)
 endif()
+if (onnxruntime_USE_TIDL)
+  add_definitions(-DUSE_TIDL=1)
+  file(GLOB_RECURSE onnxruntime_providers_tidl_cc_srcs
+    "${ONNXRUNTIME_ROOT}/core/providers/tidl/*.h"
+    "${ONNXRUNTIME_ROOT}/core/providers/tidl/*.cc"
+  )
+
+  source_group(TREE ${ONNXRUNTIME_ROOT}/core FILES ${onnxruntime_providers_tidl_cc_srcs})
+  add_library(onnxruntime_providers_tidl ${onnxruntime_providers_tidl_cc_srcs})
+  onnxruntime_add_include_to_target(onnxruntime_providers_tidl onnxruntime_common onnxruntime_framework onnx onnx_proto protobuf::libprotobuf)
+  add_dependencies(onnxruntime_providers_tidl ${onnxruntime_EXTERNAL_DEPENDENCIES})
+  # set_target_properties(onnxruntime_providers_tidl PROPERTIES FOLDER "ONNXRuntime")
+  target_include_directories(onnxruntime_providers_tidl PRIVATE ${ONNXRUNTIME_ROOT} ${eigen_INCLUDE_DIRS})
+  install(DIRECTORY ${PROJECT_SOURCE_DIR}/../include/onnxruntime/core/providers/tidl  DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}/onnxruntime/core/providers)
+  set_target_properties(onnxruntime_providers_tidl PROPERTIES LINKER_LANGUAGE CXX)
+endif()
 
 if (onnxruntime_USE_ACL)
   add_definitions(-DUSE_ACL=1)
diff --git a/cmake/onnxruntime_python.cmake b/cmake/onnxruntime_python.cmake
index ce333c3ca..733d59021 100644
--- a/cmake/onnxruntime_python.cmake
+++ b/cmake/onnxruntime_python.cmake
@@ -88,6 +88,7 @@ set(onnxruntime_pybind11_state_libs
     ${PROVIDERS_CUDA}
     ${PROVIDERS_MIGRAPHX}
     ${PROVIDERS_NUPHAR}
+    ${PROVIDERS_TIDL}
     ${PROVIDERS_VITISAI}
     ${PROVIDERS_NNAPI}
     ${PROVIDERS_RKNPU}
diff --git a/cmake/onnxruntime_session.cmake b/cmake/onnxruntime_session.cmake
index 4e64f41b2..40590b141 100644
--- a/cmake/onnxruntime_session.cmake
+++ b/cmake/onnxruntime_session.cmake
@@ -15,7 +15,7 @@ onnxruntime_add_include_to_target(onnxruntime_session onnxruntime_common onnxrun
 if(onnxruntime_ENABLE_INSTRUMENT)
   target_compile_definitions(onnxruntime_session PUBLIC ONNXRUNTIME_ENABLE_INSTRUMENT)
 endif()
-target_include_directories(onnxruntime_session PRIVATE ${ONNXRUNTIME_ROOT} ${PROJECT_SOURCE_DIR}/external/json ${eigen_INCLUDE_DIRS})
+target_include_directories(onnxruntime_session PRIVATE ${ONNXRUNTIME_ROOT} ${PROJECT_SOURCE_DIR}/external/json ${eigen_INCLUDE_DIRS} )
 add_dependencies(onnxruntime_session ${onnxruntime_EXTERNAL_DEPENDENCIES})
 set_target_properties(onnxruntime_session PROPERTIES FOLDER "ONNXRuntime")
 if (onnxruntime_USE_CUDA)
diff --git a/cmake/onnxruntime_unittests.cmake b/cmake/onnxruntime_unittests.cmake
index 981f1dacb..0c86dfeb0 100644
--- a/cmake/onnxruntime_unittests.cmake
+++ b/cmake/onnxruntime_unittests.cmake
@@ -460,6 +460,7 @@ set(ONNXRUNTIME_TEST_LIBS
     ${PROVIDERS_ARMNN}
     ${PROVIDERS_ROCM}
     ${PROVIDERS_COREML}
+    ${PROVIDERS_TIDL}
     onnxruntime_optimizer
     onnxruntime_providers
     onnxruntime_util
diff --git a/csharp/test/Microsoft.ML.OnnxRuntime.EndToEndTests.Capi/CXX_Api_Sample.cpp b/csharp/test/Microsoft.ML.OnnxRuntime.EndToEndTests.Capi/CXX_Api_Sample.cpp
index cd94ff5e5..8aedd1d8b 100644
--- a/csharp/test/Microsoft.ML.OnnxRuntime.EndToEndTests.Capi/CXX_Api_Sample.cpp
+++ b/csharp/test/Microsoft.ML.OnnxRuntime.EndToEndTests.Capi/CXX_Api_Sample.cpp
@@ -109,7 +109,9 @@ int main(int argc, char* argv[]) {
   assert(input_tensor.IsTensor());
 
   // score model & input tensor, get back output tensor
-  auto output_tensors = session.Run(Ort::RunOptions{nullptr}, input_node_names.data(), &input_tensor, 1, output_node_names.data(), 1);
+  auto run_options = Ort::RunOptions();
+  run_options.SetRunLogVerbosityLevel(2);
+  auto output_tensors = session.Run(run_options, input_node_names.data(), &input_tensor, 1, output_node_names.data(), 1);
   assert(output_tensors.size() == 1 && output_tensors.front().IsTensor());
 
   // Get pointer to output tensor float values
diff --git a/csharp/test/Microsoft.ML.OnnxRuntime.EndToEndTests.Capi/makefile b/csharp/test/Microsoft.ML.OnnxRuntime.EndToEndTests.Capi/makefile
new file mode 100644
index 000000000..47f76424c
--- /dev/null
+++ b/csharp/test/Microsoft.ML.OnnxRuntime.EndToEndTests.Capi/makefile
@@ -0,0 +1,30 @@
+################################################################################
+# Paths
+ORTROOT=../../..
+
+################################################################################
+# Compiler
+CXX = g++
+CXXFLAGS = -std=c++14
+INC = -I$(ORTROOT)/include/onnxruntime/core/session/
+CXXFLAGS += $(INC)
+
+################################################################################
+# Linker
+LIBDIR = $(ORTROOT)/build/Linux/RelWithDebInfo/
+LIBS = onnxruntime
+LDFLAGS = -L$(LIBDIR) -l$(LIBS)
+
+################################################################################
+# Targets
+all: CXX_Api_Sample C_Api_Sample
+
+CXX_Api_Sample: CXX_Api_Sample.cpp
+	$(CXX) -o $@ $^ $(CXXFLAGS) $(LDFLAGS)
+
+C_Api_Sample: C_Api_Sample.cpp
+	$(CXX) -o $@ $^ $(CXXFLAGS) $(LDFLAGS)
+
+.PHONY: clean
+clean:
+	rm -rf CXX_Api_Sample C_Api_Sample
diff --git a/csharp/test/Microsoft.ML.OnnxRuntime.EndToEndTests.Capi/setenv.sh b/csharp/test/Microsoft.ML.OnnxRuntime.EndToEndTests.Capi/setenv.sh
new file mode 100755
index 000000000..d37cd2e94
--- /dev/null
+++ b/csharp/test/Microsoft.ML.OnnxRuntime.EndToEndTests.Capi/setenv.sh
@@ -0,0 +1,12 @@
+#!/bin/bash
+
+################################################################################
+# Ensure script is only run sourced
+if [[ $_ == $0 ]]; then
+    echo "!!!ERROR!!! - PLEASE RUN WITH 'source'"
+    exit 1
+fi
+
+################################################################################
+# Set environment variables
+export LD_LIBRARY_PATH=/home/brandon/work/ecplr/onnxruntime/build/Linux/RelWithDebInfo/
diff --git a/include/onnxruntime/core/framework/execution_provider.h b/include/onnxruntime/core/framework/execution_provider.h
index 02c0622b4..934b45bbe 100644
--- a/include/onnxruntime/core/framework/execution_provider.h
+++ b/include/onnxruntime/core/framework/execution_provider.h
@@ -39,11 +39,13 @@ using MemoryInfoSet = std::set<OrtMemoryInfo>;
 using CreateFunctionStateFunc = std::function<int(ComputeContext*, FunctionState*)>;
 using ComputeFunc = std::function<Status(FunctionState, const OrtApi*, OrtKernelContext*)>;
 using DestroyFunctionStateFunc = std::function<void(FunctionState)>;
+using GetCustomData = std::function<Status(FunctionState, char **node_name, void **node_data)>;
 
 struct NodeComputeInfo {
   CreateFunctionStateFunc create_state_func;
   ComputeFunc compute_func;
   DestroyFunctionStateFunc release_state_func;
+  GetCustomData custom_func;
 };
 
 class IExecutionProvider {
diff --git a/include/onnxruntime/core/framework/func_api.h b/include/onnxruntime/core/framework/func_api.h
index 93b52d8a3..cb4fb04e2 100644
--- a/include/onnxruntime/core/framework/func_api.h
+++ b/include/onnxruntime/core/framework/func_api.h
@@ -24,4 +24,8 @@ using CreateFunctionStateC = int (*)(ComputeContext*, FunctionState*);
 using ComputeFuncC = common::Status (*)(FunctionState, const OrtApi*, OrtKernelContext*);
 // release the function state.
 using DestroyFunctionStateC = void (*)(FunctionState);
+
+// Custom Data re interpret
+using CustomFuncC = Status (*)(FunctionState, char **node_name, void **node_data);
+
 }  // namespace onnxruntime
diff --git a/include/onnxruntime/core/graph/constants.h b/include/onnxruntime/core/graph/constants.h
index 8ce6147c0..505948e87 100644
--- a/include/onnxruntime/core/graph/constants.h
+++ b/include/onnxruntime/core/graph/constants.h
@@ -37,4 +37,6 @@ constexpr const char* kArmNNExecutionProvider = "ArmNNExecutionProvider";
 constexpr const char* kRocmExecutionProvider = "ROCMExecutionProvider";
 constexpr const char* kCoreMLExecutionProvider = "CoreMLExecutionProvider";
 
+constexpr const char* kTidlExecutionProvider = "TIDLExecutionProvider";
+constexpr const char* kTidlCompilationProvider = "TIDLCompilationProvider";
 }  // namespace onnxruntime
diff --git a/include/onnxruntime/core/providers/dnnl/dnnl_provider_factory.h b/include/onnxruntime/core/providers/dnnl/dnnl_provider_factory.h
index b87780cb0..dbee2f56d 100644
--- a/include/onnxruntime/core/providers/dnnl/dnnl_provider_factory.h
+++ b/include/onnxruntime/core/providers/dnnl/dnnl_provider_factory.h
@@ -1,7 +1,8 @@
 // Copyright (c) Microsoft Corporation. All rights reserved.
 // Licensed under the MIT License.
 
-#include "onnxruntime_c_api.h"
+#include <onnxruntime_c_api.h>
+/* #include <onnxruntime/core/session/onnxruntime_cxx_api.h> */
 
 #ifdef __cplusplus
 extern "C" {
diff --git a/include/onnxruntime/core/providers/tidl/tidl_provider_factory.h b/include/onnxruntime/core/providers/tidl/tidl_provider_factory.h
new file mode 100644
index 000000000..cafea9689
--- /dev/null
+++ b/include/onnxruntime/core/providers/tidl/tidl_provider_factory.h
@@ -0,0 +1,24 @@
+// Copyright 2019 JD.com Inc. JD AI
+
+#include "onnxruntime_c_api.h"
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+typedef struct
+{
+  char import[512];
+  int debug_level;
+  int tidl_tensor_bits;
+  char tidl_tools_path[512];
+  char artifacts_folder[512];
+} c_api_tidl_options;
+
+ORT_API_STATUS(OrtSessionOptionsAppendExecutionProvider_Tidl, _In_ OrtSessionOptions* options, c_api_tidl_options * tidl_options);
+
+#ifdef __cplusplus
+}
+#endif
+
+
diff --git a/onnxruntime/core/framework/func_kernel.h b/onnxruntime/core/framework/func_kernel.h
index ece2c2430..15e8082d0 100644
--- a/onnxruntime/core/framework/func_kernel.h
+++ b/onnxruntime/core/framework/func_kernel.h
@@ -42,6 +42,17 @@ class FunctionKernel : public OpKernel {
                                        reinterpret_cast<OrtKernelContext*>(context_internal));
   }
 
+  virtual Status Custom(char **node_name, void **node_data) const {
+    if(compute_info_->custom_func)
+    {
+      return compute_info_->custom_func(func_state_, node_name, node_data);
+    }
+    else
+    {
+      return ORT_MAKE_STATUS(ONNXRUNTIME, FAIL, "No Custon Data \n");
+    }
+  }
+
  private:
   NodeComputeInfo* compute_info_{nullptr};
   FunctionState func_state_{nullptr};
diff --git a/onnxruntime/core/framework/fuse_nodes_funcs.cc b/onnxruntime/core/framework/fuse_nodes_funcs.cc
index f4678afbe..2becd87f3 100644
--- a/onnxruntime/core/framework/fuse_nodes_funcs.cc
+++ b/onnxruntime/core/framework/fuse_nodes_funcs.cc
@@ -43,6 +43,11 @@ Status FuncManager::GetFuncs(const std::string& name, NodeComputeInfo*& compute_
     ORT_RETURN_IF_ERROR(Env::Default().GetSymbolFromLibrary(handle,
                                                             kReleaseStateFuncSymbol + name,
                                                             &release_func_symbol_handle));
+    void* custom_func_symbol_handle = nullptr;
+    ORT_RETURN_IF_ERROR(Env::Default().GetSymbolFromLibrary(handle,
+                                                            kGetCustomDataFuncSymbol + name,
+                                                            &custom_func_symbol_handle));
+
     it->second.compute_info.compute_func = [=](FunctionState state, const OrtApi* api, OrtKernelContext* context) {
       return reinterpret_cast<ComputeFuncC>(compute_func_symbol_handle)(state, api, context);
     };
@@ -54,6 +59,12 @@ Status FuncManager::GetFuncs(const std::string& name, NodeComputeInfo*& compute_
     it->second.compute_info.release_state_func = [=](FunctionState state) {
       return reinterpret_cast<DestroyFunctionStateC>(release_func_symbol_handle)(state);
     };
+
+    it->second.compute_info.custom_func = [=](FunctionState state, char **node_name, void **node_data) {
+      return reinterpret_cast<CustomFuncC>(custom_func_symbol_handle)(state, node_name, node_data);
+    };
+
+
   }
 
   compute_info = &it->second.compute_info;
diff --git a/onnxruntime/core/framework/fuse_nodes_funcs.h b/onnxruntime/core/framework/fuse_nodes_funcs.h
index aa3dc0162..1747c2c31 100644
--- a/onnxruntime/core/framework/fuse_nodes_funcs.h
+++ b/onnxruntime/core/framework/fuse_nodes_funcs.h
@@ -33,6 +33,7 @@ class FuncManager {
   const std::string kComputeFuncSymbol = "Compute_";
   const std::string kCreateStateFuncSymbol = "Create_State_";
   const std::string kReleaseStateFuncSymbol = "Release_State_";
+  const std::string kGetCustomDataFuncSymbol = "Custom_State_";
 
   // note that subgraph session state shares fused_funcs with main graph
   // because it's filled in by the time main graph is traversed,
diff --git a/onnxruntime/core/framework/graph_partitioner.cc b/onnxruntime/core/framework/graph_partitioner.cc
index 9a17ffe28..a752055ba 100644
--- a/onnxruntime/core/framework/graph_partitioner.cc
+++ b/onnxruntime/core/framework/graph_partitioner.cc
@@ -249,6 +249,10 @@ static Status PartitionOnnxFormatModelImpl(Graph& graph, bool export_dll, FuncMa
         }
 
         for (size_t j = 0, end = nodes_to_compile.size(); j < end; j++) {
+          if(nodes_to_compile[j]->OpType() != "TIDL_0")
+          {
+            node_compute_funcs[j].custom_func = NULL;
+          }
           ORT_RETURN_IF_ERROR(func_mgr.AddFuncInfo(nodes_to_compile[j]->Name(), std::move(node_compute_funcs[j])));
         }
       }
diff --git a/onnxruntime/core/framework/utils.cc b/onnxruntime/core/framework/utils.cc
index 94f1f3f56..62e31b51f 100644
--- a/onnxruntime/core/framework/utils.cc
+++ b/onnxruntime/core/framework/utils.cc
@@ -101,6 +101,8 @@ bool ProviderIsCpuBased(const std::string& provider_type) {
          provider_type == onnxruntime::kOpenVINOExecutionProvider ||
          provider_type == onnxruntime::kNnapiExecutionProvider ||
          provider_type == onnxruntime::kAclExecutionProvider ||
+         provider_type == onnxruntime::kTidlExecutionProvider ||
+         provider_type == onnxruntime::kTidlCompilationProvider ||
          provider_type == onnxruntime::kArmNNExecutionProvider ||
          provider_type == onnxruntime::kRknpuExecutionProvider ||
          provider_type == onnxruntime::kCoreMLExecutionProvider ||
diff --git a/onnxruntime/core/providers/get_execution_providers.cc b/onnxruntime/core/providers/get_execution_providers.cc
index 866d71fee..809a52efa 100644
--- a/onnxruntime/core/providers/get_execution_providers.cc
+++ b/onnxruntime/core/providers/get_execution_providers.cc
@@ -63,6 +63,22 @@ constexpr ProviderInfo kProvidersInPriorityOrder[] =
             true,
 #else
             false,
+#endif
+        },
+        {
+            kTidlExecutionProvider,
+#ifdef USE_TIDL
+            true,
+#else
+            false,
+#endif
+        },
+        {
+            kTidlCompilationProvider,
+#ifdef USE_TIDL
+            true,
+#else
+            false,
 #endif
         },
         {
diff --git a/onnxruntime/core/providers/tidl/symbols.txt b/onnxruntime/core/providers/tidl/symbols.txt
new file mode 100644
index 000000000..27ab3ca3c
--- /dev/null
+++ b/onnxruntime/core/providers/tidl/symbols.txt
@@ -0,0 +1 @@
+OrtSessionOptionsAppendExecutionProvider_Tidl
diff --git a/onnxruntime/core/providers/tidl/tidl_execution_provider.cc b/onnxruntime/core/providers/tidl/tidl_execution_provider.cc
new file mode 100644
index 000000000..c2bbd2c45
--- /dev/null
+++ b/onnxruntime/core/providers/tidl/tidl_execution_provider.cc
@@ -0,0 +1,449 @@
+// Copyright (c) Microsoft Corporation. All rights reserved.
+// Licensed under the MIT License.
+
+
+#include "tidl_execution_provider.h"
+#include "core/framework/allocatormgr.h"
+#include "core/framework/compute_capability.h"
+#include "core/session/onnxruntime_cxx_api.h"
+#include "core/session/inference_session.h"
+#include "core/graph/model.h"
+#include "float.h"
+#include "math.h"
+
+#include <string>
+
+
+//#define TIDL_IMPORT_ONNX
+
+namespace onnxruntime {
+
+constexpr const char* TIDL = "Tidl";
+constexpr const char* TIDL_CPU = "TidlCpu";
+
+
+TidlExecutionProvider::TidlExecutionProvider(const TidlExecutionProviderInfo& info)
+    : IExecutionProvider{onnxruntime::kTidlExecutionProvider} {
+  AllocatorCreationInfo default_memory_info{
+      [](int) {
+        return onnxruntime::make_unique<CPUAllocator>(OrtMemoryInfo(TIDL, OrtAllocatorType::OrtDeviceAllocator));
+      },
+      0};
+
+   AllocatorCreationInfo cpu_memory_info{
+      [](int) {
+        return onnxruntime::make_unique<CPUAllocator>(
+            OrtMemoryInfo(TIDL_CPU, OrtAllocatorType::OrtDeviceAllocator, OrtDevice(), 0, OrtMemTypeCPUOutput));
+      }};
+
+  InsertAllocator(CreateAllocator(default_memory_info));
+  InsertAllocator(CreateAllocator(cpu_memory_info));  
+  TIDLProviderOptions interface_options = info.options_tidl_onnx_vec;
+  
+  is_import_ = (info.type == "TIDLCompilationProvider");
+
+  if(is_import_)
+  {
+    std::string tidl_tools_path;
+    for (auto _ : info.options_tidl_onnx_vec) {
+      auto key = _.first;
+      auto value = _.second;
+      if(key == "tidl_tools_path")
+        tidl_tools_path = value;
+    }
+    tidl_ops_->lib = dlopen((tidl_tools_path + "/tidl_model_import_onnx.so").c_str(), RTLD_NOW | RTLD_GLOBAL);
+    if(! tidl_ops_->lib)
+    {
+      printf("Error -   %s \n", dlerror());
+    }
+    assert(tidl_ops_->lib);
+  }
+  else
+  {
+    tidl_ops_->lib = dlopen("libtidl_onnxrt_EP.so", RTLD_NOW | RTLD_GLOBAL);
+    if(! tidl_ops_->lib)
+    {
+      printf("Error -   %s \n", dlerror());
+    }
+    printf("libtidl_onnxrt_EP loaded %p \n", tidl_ops_->lib);
+    assert(tidl_ops_->lib);
+  }
+  tidl_ops_->TIDL_getSupportedNodes = reinterpret_cast<decltype(tidl_ops_->TIDL_getSupportedNodes)>(dlsym(tidl_ops_->lib, "TIDL_getSupportedNodes"));
+  tidl_ops_->TIDL_populateOptions = reinterpret_cast<decltype(tidl_ops_->TIDL_populateOptions)>(dlsym(tidl_ops_->lib, "TIDL_populateOptions"));
+  tidl_ops_->TIDL_createStateFunc = reinterpret_cast<decltype(tidl_ops_->TIDL_createStateFunc)>(dlsym(tidl_ops_->lib, "TIDL_createStateFunc"));
+  tidl_ops_->TIDL_computeImportFunc = reinterpret_cast<decltype(tidl_ops_->TIDL_computeImportFunc)>(dlsym(tidl_ops_->lib, "TIDL_computeImportFunc"));
+  tidl_ops_->TIDL_computeInvokeFunc = reinterpret_cast<decltype(tidl_ops_->TIDL_computeInvokeFunc)>(dlsym(tidl_ops_->lib, "TIDL_computeInvokeFunc"));
+  tidl_ops_->TIDL_releaseRtFunc = reinterpret_cast<decltype(tidl_ops_->TIDL_releaseRtFunc)>(dlsym(tidl_ops_->lib, "TIDL_releaseRtFunc"));
+  tidl_ops_->TIDL_isInputConst = reinterpret_cast<decltype(tidl_ops_->TIDL_isInputConst)>(dlsym(tidl_ops_->lib, "TIDL_isInputConst"));
+  tidl_ops_->TIDL_getOutputShape = reinterpret_cast<decltype(tidl_ops_->TIDL_getOutputShape)>(dlsym(tidl_ops_->lib, "TIDL_getOutputShape"));
+  tidl_ops_->TIDLEP_getDdrStats = reinterpret_cast<decltype(tidl_ops_->TIDLEP_getDdrStats)>(dlsym(tidl_ops_->lib, "TIDLEP_getDdrStats"));
+  tidl_ops_->TIDLEP_getSubGraphStats = reinterpret_cast<decltype(tidl_ops_->TIDLEP_getSubGraphStats)>(dlsym(tidl_ops_->lib, "TIDLEP_getSubGraphStats"));
+
+  bool status = false;
+  status = tidl_ops_->TIDL_populateOptions(interface_options);
+  // TODO : how to pass error if status is false?
+}
+
+TidlExecutionProvider::~TidlExecutionProvider() {
+  //TODO : Add delete here
+}
+
+
+int32_t TidlExecutionProvider::GetCustomMemStats(uint64_t * read, uint64_t * write) const {
+
+  return (tidl_ops_->TIDLEP_getDdrStats(read, write));
+}
+
+std::vector<std::unique_ptr<ComputeCapability>>
+TidlExecutionProvider::GetCapability(const onnxruntime::GraphViewer& graph,
+                                      const std::vector<const KernelRegistry*>& /*kernel_registries*/) const {
+   // check if all nodes have shape for Input
+#if 0
+  for (const auto& node : graph.Nodes()) 
+  {
+    for(auto& def : node.InputDefs())
+    {
+      const auto& tensorShape = def->Shape();
+      if( tensorShape )
+      {
+        for (int i = 0; i < tensorShape->dim_size(); i++)
+        {
+          printf(" Shape %d of %s is %lld \n", i, node.Name().c_str(), tensorShape->dim(i).dim_value());
+        }
+      }
+      else
+      {
+        printf(" Shape not foud for %s\n", node.Name().c_str());
+      }
+    }
+  }
+#endif
+  // This method is based on that of TRT EP
+  // Construct modelproto from graph
+  //OnnxTIDLSubGraphParams *state_subGraph = (OnnxTIDLSubGraphParams*)malloc(sizeof(OnnxTIDLSubGraphParams));
+   // Dump model Proto to file to pass it to pyxir
+  auto logger = *GetLogger();
+
+  const Graph& node_graph = graph.GetGraph();
+  const std::string& name_ = node_graph.Name();
+  onnxruntime::Model model{name_, true, ModelMetaData{}, PathString{},
+                           IOnnxRuntimeOpSchemaRegistryList{},
+                           node_graph.DomainToVersionMap(),
+                           std::vector<ONNX_NAMESPACE::FunctionProto>(),
+                           logger};
+
+  ONNX_NAMESPACE::ModelProto model_proto = model.ToProto();
+  model_proto.set_ir_version(ONNX_NAMESPACE::Version::IR_VERSION);
+
+  *(model_proto.mutable_graph()) = node_graph.ToGraphProto();
+
+  onnx::GraphProto onnxGraph = model_proto.graph();
+  
+  std::string string_buf;
+  string_buf = model_proto.SerializeAsString();
+
+  const auto supported_nodes_vector = tidl_ops_->TIDL_getSupportedNodes(string_buf, node_graph.DomainToVersionMap().at(kOnnxDomain));
+
+  onnxruntime::Graph& graph_build = model.MainGraph();
+  const std::vector<NodeIndex>& node_index = graph.GetNodesInTopologicalOrder();
+  std::set<NodeArg*> all_node_inputs;
+  for (const auto& node : graph.Nodes()) {
+    std::vector<onnxruntime::NodeArg*> inputs, outputs;
+    for (auto input : node.InputDefs()) {
+      auto& n_input = graph_build.GetOrCreateNodeArg(input->Name(), input->TypeAsProto());
+      inputs.push_back(&n_input);
+      all_node_inputs.insert(&n_input);
+    }
+    for (auto output : node.OutputDefs()) {
+      auto& n_output = graph_build.GetOrCreateNodeArg(output->Name(), output->TypeAsProto());
+      outputs.push_back(&n_output);
+    }
+    graph_build.AddNode(node.Name(), node.OpType(), node.Description(), inputs, outputs, &node.GetAttributes(), node.Domain());
+  }
+  const auto graph_outputs = graph.GetOutputs();
+  //Add initializer to graph
+  const auto& init_tensors = graph.GetAllInitializedTensors();
+  for (const auto& tensor : init_tensors) {
+    graph_build.AddInitializedTensor(*(tensor.second));
+  }
+
+  ORT_ENFORCE(graph_build.Resolve().IsOK());
+
+  std::unique_ptr<IndexedSubGraph> sub_graph = onnxruntime::make_unique<IndexedSubGraph>();
+
+  // Find inputs, initializers and outputs for each supported subgraph
+  std::vector<std::unique_ptr<ComputeCapability>> result;
+
+  int counter = 0;
+
+  for (const auto& group : supported_nodes_vector) {
+    if (!group.empty()) {
+      std::unordered_set<size_t> node_set;
+      node_set.reserve(group.size());
+      for (const auto& index : group) {
+        node_set.insert(node_index[index]);
+      }
+      std::unique_ptr<IndexedSubGraph> sub_graph = onnxruntime::make_unique<IndexedSubGraph>();
+      // Find inputs and outputs of the subgraph
+      std::unordered_map<const NodeArg*, int> fused_inputs, fused_outputs, fused_outputs_to_add;
+      std::unordered_set<const NodeArg*> erased;
+      int input_order = 0;
+      int output_order = 0;
+
+      for (const auto& index : group) {
+        sub_graph->nodes.push_back(node_index[index]);
+        const auto& node = graph.GetNode(node_index[index]);
+
+        for (const auto& input : node->InputDefs()) {
+          const auto& it = fused_outputs.find(input);
+
+          if (it != fused_outputs.end()) {
+            fused_outputs.erase(it);
+            erased.insert(input);
+          }
+          //only when input is neither in output list nor erased list, add the input to input list
+          else if (erased.find(input) == erased.end()) {
+            fused_inputs[input] = input_order++;
+          }
+        }
+
+        // For output searching, there is a special case:
+        // If node's OutputEdges are more than its outputs, meaning certain output is used more than once,
+        // if the output is connected to nodes that don't belong to the subgraph, the output need to be added
+        // to the output list
+        if (node->GetOutputEdgesCount() > node->OutputDefs().size()) {
+          for (auto it = node->OutputEdgesBegin(), end = node->OutputEdgesEnd(); it != end; ++it) {
+            const auto& node_idx = it->GetNode().Index();
+            const auto& output = (it->GetNode()).InputDefs()[it->GetDstArgIndex()];
+
+            if (node_set.find(node_idx) != node_set.end()) {
+              const auto& iter = fused_inputs.find(output);
+
+              if (iter != fused_inputs.end()) {
+                fused_inputs.erase(iter);
+                erased.insert(output);
+              } else if (erased.find(output) == erased.end()) {
+                fused_outputs[output] = output_order++;
+              }
+            } else {
+              fused_outputs_to_add[output] = output_order++;
+            }
+          }
+        } else {
+          for (const auto& output : node->OutputDefs()) {
+            const auto& it = fused_inputs.find(output);
+
+            if (it != fused_inputs.end()) {
+              fused_inputs.erase(it);
+              erased.insert(output);
+            }
+            // only when output is neither in input list nor erased list, add the output to output list
+            else if (erased.find(output) == erased.end()) {
+              fused_outputs[output] = output_order++;
+            }
+          }
+        }
+      }
+
+      fused_outputs.insert(fused_outputs_to_add.begin(), fused_outputs_to_add.end());
+
+      // Sort inputs and outputs by the order they were added
+      std::multimap<int, const NodeArg*> inputs, outputs;
+
+      for (auto it = fused_inputs.begin(), end = fused_inputs.end(); it != end; ++it) {
+        inputs.insert(std::pair<int, const NodeArg*>(it->second, it->first));
+      }
+
+      for (auto it = fused_outputs.begin(), end = fused_outputs.end(); it != end; ++it) {
+        for (const auto& x : all_node_inputs) {
+          if (x->Name() == it->first->Name()) {
+            outputs.insert(std::pair<int, const NodeArg*>(it->second, it->first));
+            break;
+          }
+        }
+        if (std::find(graph_outputs.begin(), graph_outputs.end(), it->first) != graph_outputs.end()) {
+          outputs.insert(std::pair<int, const NodeArg*>(it->second, it->first));
+        }
+      }
+
+      // Assign inputs and outputs to subgraph's meta_def
+      auto meta_def = onnxruntime::make_unique<::onnxruntime::IndexedSubGraph::MetaDef>();
+      meta_def->name = "TIDL_" + std::to_string(counter++);
+      meta_def->domain = kMSDomain;
+
+      for (const auto& input : inputs) {
+        meta_def->inputs.push_back(input.second->Name());
+      }
+
+      for (const auto& output : outputs) {
+        meta_def->outputs.push_back(output.second->Name());
+      }
+
+      meta_def->since_version = 1;
+      sub_graph->SetMetaDef(std::move(meta_def));
+
+      result.push_back(onnxruntime::make_unique<ComputeCapability>(std::move(sub_graph)));
+    }
+  }
+  return result;
+}
+void populateOnnxRtInputParams(Ort::CustomOpApi ort, OrtKernelContext * context, 
+                          tidl_ops * tidl_ops, OnnxTIDLSubGraphParams * state_subGraph)
+{
+  int32_t i, currInIdx = 0; 
+  onnxRtParams_t * onnxRtParams = &state_subGraph->onnxRtParams;
+  // populate input params  
+  for (i = 0; i < state_subGraph->numInputs; i++) 
+  {    
+    const OrtValue* input_tensor = ort.KernelContext_GetInput(context, state_subGraph->inputIdx[i]);
+    OrtTensorTypeAndShapeInfo* input_tensor_info = ort.GetTensorTypeAndShape(input_tensor);
+    int64_t inTensorElementType = ort.GetTensorElementType(input_tensor_info);
+    const auto& tensor_shape = ort.GetTensorShape(input_tensor_info);
+    ort.ReleaseTensorTypeAndShapeInfo(input_tensor_info);
+
+    void * input;
+    if (inTensorElementType == ONNX_TENSOR_ELEMENT_DATA_TYPE_UINT8)
+    {
+      input = const_cast<uint8_t*>(ort.GetTensorData<uint8_t>(input_tensor));
+    }
+    else if (inTensorElementType == ONNX_TENSOR_ELEMENT_DATA_TYPE_INT32)
+    {
+      input = const_cast<int32_t*>(ort.GetTensorData<int32_t>(input_tensor));
+    }
+    else if (inTensorElementType == ONNX_TENSOR_ELEMENT_DATA_TYPE_INT64)
+    {
+      input = const_cast<int64_t*>(ort.GetTensorData<int64_t>(input_tensor));
+    }
+    else if (inTensorElementType == ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT)
+    {
+      input = const_cast<float*>(ort.GetTensorData<float>(input_tensor));
+    }
+    else
+    {
+      printf("ERROR : Unsupported input_tensor element type %d \n", inTensorElementType);
+    }
+
+    onnxRtParams->inputTensorData[currInIdx] = (void *)input; 
+    onnxRtParams->inputTensorElementType[currInIdx] = inTensorElementType;
+    onnxRtParams->tensorShape[currInIdx][3] = tensor_shape[3];
+    onnxRtParams->tensorShape[currInIdx][2] = tensor_shape[2];
+    onnxRtParams->tensorShape[currInIdx][1] = tensor_shape[1];
+    onnxRtParams->tensorShape[currInIdx][0] = tensor_shape[0];
+    currInIdx++;
+  }
+  onnxRtParams->numNetInData = state_subGraph->numInputs;
+  onnxRtParams->numNetOutData = state_subGraph->numOutputs;
+}
+
+void populateOnnxRtOutputParams(Ort::CustomOpApi ort, OrtKernelContext * context, tidl_ops * tidl_ops, OnnxTIDLSubGraphParams * state_subGraph)
+{
+  onnxRtParams_t * onnxRtParams = &state_subGraph->onnxRtParams;
+  //populate output params
+  for (int j = 0; j < onnxRtParams->numNetOutData; j++) 
+  {
+    std::vector<int64_t> nchw_shape = tidl_ops->TIDL_getOutputShape(state_subGraph->ioBufDesc, onnxRtParams->outDataNames[j]);
+    auto* output_tensor = ort.KernelContext_GetOutput(context, j, nchw_shape.data(), nchw_shape.size());
+    OrtTensorTypeAndShapeInfo* output_info = ort.GetTensorTypeAndShape(output_tensor);
+    int64_t outTensorElementType = ort.GetTensorElementType(output_info);
+    ort.ReleaseTensorTypeAndShapeInfo(output_info);   
+    void * output;
+    if (outTensorElementType == ONNX_TENSOR_ELEMENT_DATA_TYPE_UINT8)
+    {
+      output = ort.GetTensorMutableData<uint8_t>(output_tensor);
+    }
+    else if (outTensorElementType == ONNX_TENSOR_ELEMENT_DATA_TYPE_INT32)
+    {
+      output = ort.GetTensorMutableData<int32_t>(output_tensor);
+    }
+    else if (outTensorElementType == ONNX_TENSOR_ELEMENT_DATA_TYPE_INT64)
+    {
+      output = ort.GetTensorMutableData<int64_t>(output_tensor);
+    }
+    else if (outTensorElementType == ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT)
+    {
+      output = ort.GetTensorMutableData<float>(output_tensor);
+    }
+    else
+    {
+      printf("ERROR : Unsupported output tensor element type %d \n", outTensorElementType);
+    }
+    onnxRtParams->outputTensorData[j] = (void *)output; 
+    onnxRtParams->outputTensorElementType[j] = outTensorElementType;
+  }
+}
+
+common::Status TidlExecutionProvider::Compile(const std::vector<onnxruntime::Node*>& fused_nodes,
+                                               std::vector<NodeComputeInfo>& node_compute_funcs) {
+   for (const auto* fused_node : fused_nodes) {
+    // Reconstruct graph proto from fused node's function body
+    const auto* func_body = fused_node->GetFunctionBody();
+    if (!func_body) {
+      return common::Status(common::ONNXRUNTIME, common::INVALID_ARGUMENT, "Function body is empty");
+    }
+    const Graph& graph_body = func_body->Body();
+    onnxruntime::Model model(graph_body.Name(), true, ModelMetaData(), PathString(),
+                             IOnnxRuntimeOpSchemaRegistryList(), graph_body.DomainToVersionMap(),
+                             std::vector<ONNX_NAMESPACE::FunctionProto>(), *GetLogger());
+    ONNX_NAMESPACE::ModelProto* model_proto = new ONNX_NAMESPACE::ModelProto();
+    *model_proto = model.ToProto();
+    *(model_proto->mutable_graph()) = graph_body.ToGraphProto();
+    model_proto->set_ir_version(ONNX_NAMESPACE::Version::IR_VERSION);
+
+    std::string * string_buf = new std::string();
+    *string_buf = model_proto->SerializeAsString();
+
+
+    model_protos_.emplace(fused_node->Name(), string_buf);
+
+    NodeComputeInfo compute_info;
+    
+    compute_info.create_state_func = [&](ComputeContext* context, FunctionState* state) 
+    {
+      OnnxTIDLSubGraphParams *state_subGraph = (OnnxTIDLSubGraphParams*)malloc(sizeof(OnnxTIDLSubGraphParams));
+      std::string * string_buf = model_protos_[context->node_name];
+
+      tidl_ops_->TIDL_createStateFunc(state_subGraph, string_buf, context->node_name);
+
+      *state = state_subGraph;
+
+      return 0;      
+    };
+
+    compute_info.release_state_func = [&](FunctionState state) 
+    {
+       OnnxTIDLSubGraphParams *state_subGraph = reinterpret_cast<OnnxTIDLSubGraphParams*>(state);
+       tidl_ops_->TIDL_releaseRtFunc(state_subGraph);
+       free(state);
+    };
+
+    compute_info.compute_func = [&](FunctionState state, const OrtCustomOpApi* api, OrtKernelContext* context) 
+    {
+      OnnxTIDLSubGraphParams *state_subGraph = reinterpret_cast<OnnxTIDLSubGraphParams*>(state);
+      Ort::CustomOpApi ort{*api};
+
+      populateOnnxRtInputParams(ort, context, tidl_ops_, state_subGraph);
+      if(is_import_)
+      {
+        std::string * string_buf = reinterpret_cast<std::string *>(state_subGraph->string_buf);
+        tidl_ops_->TIDL_computeImportFunc(state_subGraph, string_buf, graph_body.DomainToVersionMap().at(kOnnxDomain));
+      }
+      populateOnnxRtOutputParams(ort, context, tidl_ops_, state_subGraph);
+      tidl_ops_->TIDL_computeInvokeFunc(state_subGraph);
+
+      return Status::OK();
+
+    };
+ 
+    compute_info.custom_func = [&](FunctionState state , char **node_name, void **node_data) 
+    {
+      OnnxTIDLSubGraphParams *state_subGraph = reinterpret_cast<OnnxTIDLSubGraphParams*>(state);
+
+      tidl_ops_->TIDLEP_getSubGraphStats(state_subGraph, node_name, node_data);
+      return (Status::OK());
+    };
+
+    node_compute_funcs.push_back(compute_info);
+  };
+  return Status::OK();
+}
+}  // namespace onnxruntime
diff --git a/onnxruntime/core/providers/tidl/tidl_execution_provider.h b/onnxruntime/core/providers/tidl/tidl_execution_provider.h
new file mode 100644
index 000000000..98f95030a
--- /dev/null
+++ b/onnxruntime/core/providers/tidl/tidl_execution_provider.h
@@ -0,0 +1,79 @@
+// Copyright (c) Microsoft Corporation. All rights reserved.
+// Licensed under the MIT License.
+
+
+#pragma once
+
+#include <vector>
+#include <map>
+#include <sstream>
+#include <cstring>
+#include <algorithm>
+#include <dlfcn.h>
+#include <cmath>
+#include <float.h>
+#include <errno.h>
+#include <sys/stat.h>
+#include <dirent.h>
+
+#include "core/framework/execution_provider.h"
+#include "core/graph/onnx_protobuf.h"
+
+#include "tidl_execution_provider_common.h"
+
+
+#define TIDL_STRING_SIZE        ((int32_t) 512)
+#define TIDL_MAX_ALG_IN_BUFS    ((int32_t) 32)
+#define TIDL_MAX_ALG_OUT_BUFS   ((int32_t) 32)
+
+#define DEFAULT_COMPILE_CONSTRAINT_NC_FLAGS (0x1 | 0x40 | 0x200 | 0x400)
+
+using TIDLProviderOptions = std::vector<std::pair<std::string,std::string>>;
+
+namespace onnxruntime {
+
+typedef  struct 
+{
+    void *lib;
+    decltype(&::TIDL_getSupportedNodes) TIDL_getSupportedNodes;
+    decltype(&::TIDL_populateOptions) TIDL_populateOptions;
+    decltype(&::TIDL_createStateFunc) TIDL_createStateFunc;
+    decltype(&::TIDL_computeImportFunc) TIDL_computeImportFunc;
+    decltype(&::TIDL_computeInvokeFunc) TIDL_computeInvokeFunc;
+    decltype(&::TIDL_releaseRtFunc) TIDL_releaseRtFunc;
+    decltype(&::TIDL_isInputConst) TIDL_isInputConst;
+    decltype(&::TIDL_getOutputShape) TIDL_getOutputShape;
+    decltype(&::TIDLEP_getDdrStats) TIDLEP_getDdrStats;
+    decltype(&::TIDLEP_getSubGraphStats) TIDLEP_getSubGraphStats;
+} tidl_ops;
+
+// Information needed to construct TIDL execution providers.
+struct TidlExecutionProviderInfo {
+  TIDLProviderOptions options_tidl_onnx_vec;
+  std::string type;
+
+  explicit TidlExecutionProviderInfo(const std::string& type, const TIDLProviderOptions& in_options_tidl_onnx_vec)
+      : options_tidl_onnx_vec(in_options_tidl_onnx_vec), type(type) {}
+  TidlExecutionProviderInfo() = default;
+};
+
+class TidlExecutionProvider : public IExecutionProvider {
+ public:
+  TidlExecutionProvider(const TidlExecutionProviderInfo& info);
+  virtual ~TidlExecutionProvider();
+
+  std::vector<std::unique_ptr<ComputeCapability>>
+  GetCapability(const onnxruntime::GraphViewer& graph,
+                const std::vector<const KernelRegistry*>& /*kernel_registries*/) const override;
+  common::Status Compile(const std::vector<onnxruntime::Node*>& fused_nodes,
+                         std::vector<NodeComputeInfo>& node_compute_funcs) override;
+  
+  int32_t GetCustomMemStats(uint64_t * read, uint64_t * write) const; 
+
+ private:
+  std::unordered_map<std::string, std::string*> model_protos_;
+  tidl_ops * tidl_ops_ = new tidl_ops;
+  int32_t is_import_;
+  
+};
+}  // namespace onnxruntime
diff --git a/onnxruntime/core/providers/tidl/tidl_execution_provider_common.h b/onnxruntime/core/providers/tidl/tidl_execution_provider_common.h
new file mode 100644
index 000000000..88db89b97
--- /dev/null
+++ b/onnxruntime/core/providers/tidl/tidl_execution_provider_common.h
@@ -0,0 +1,64 @@
+// Copyright (c) Microsoft Corporation. All rights reserved.
+// Licensed under the MIT License.
+#ifndef TIDL_ONNX_RT_EP_COMMON_H
+#define TIDL_ONNX_RT_EP_COMMON_H 1
+
+#include <stdint.h>
+#include <stdarg.h>
+#include <string.h>
+#include <stdlib.h>
+
+using std::string;
+using std::vector;
+
+#define TIDL_STRING_SIZE        ((int32_t) 512)
+#define TIDL_MAX_ALG_IN_BUFS    ((int32_t) 32)
+#define TIDL_MAX_ALG_OUT_BUFS   ((int32_t) 32)
+
+typedef struct {
+  int32_t numNetInData;
+  int32_t numNetOutData;
+  int32_t tensorShape[TIDL_MAX_ALG_IN_BUFS][4];
+  int8_t  inDataNames[TIDL_MAX_ALG_IN_BUFS][TIDL_STRING_SIZE];
+  int8_t  outDataNames[TIDL_MAX_ALG_OUT_BUFS][TIDL_STRING_SIZE];
+  void *  inputTensorData[TIDL_MAX_ALG_IN_BUFS];
+  void *  outputTensorData[TIDL_MAX_ALG_IN_BUFS];
+  int64_t inputTensorElementType[TIDL_MAX_ALG_IN_BUFS];
+  int64_t outputTensorElementType[TIDL_MAX_ALG_IN_BUFS];
+} onnxRtParams_t;
+
+typedef struct 
+{
+  void* string_buf;
+  int32_t currFrameIdx_; 
+  void *subGraphPtr_;
+  char subGraphName_[100]; 
+  int32_t inputIdx[TIDL_MAX_ALG_IN_BUFS];
+  int32_t numInputs;
+  int32_t numOutputs;
+  void * rtHandle;
+  void * rtInList;
+  void * rtOutList;
+  void * stats;
+  void * netPtr;
+  void * ioBufDesc;
+  onnxRtParams_t onnxRtParams;
+}OnnxTIDLSubGraphParams;
+
+extern "C"
+{
+  bool TIDL_populateOptions(std::vector<std::pair<std::string,std::string>> interface_options);
+  std::vector<std::vector<int>> TIDL_getSupportedNodes(std::string& data, int32_t opsetVersion);
+  void TIDL_createStateFunc(OnnxTIDLSubGraphParams * state_subgraph, std::string * string_buf, const std::string node_name);
+  void TIDL_computeImportFunc(OnnxTIDLSubGraphParams * state_subGraph, std::string * string_buf, int32_t opSetVersion);
+  void TIDL_computeInvokeFunc(OnnxTIDLSubGraphParams * state_subGraph);
+  void TIDL_releaseRtFunc(OnnxTIDLSubGraphParams * state_subGraph);
+  int32_t TIDL_isInputConst(std::string * string_buf, const string name);
+  std::vector<int64_t> TIDL_getOutputShape(void * ioBufDescVPtr, int8_t onnxName[]);
+  int32_t TIDLEP_getDdrStats(uint64_t * read, uint64_t * write);
+  int32_t TIDLEP_getSubGraphStats(OnnxTIDLSubGraphParams * state_subGraph, char **node_name, void **node_data);
+
+}
+
+
+#endif
\ No newline at end of file
diff --git a/onnxruntime/core/providers/tidl/tidl_provider_factory.cc b/onnxruntime/core/providers/tidl/tidl_provider_factory.cc
new file mode 100644
index 000000000..1c0bd1659
--- /dev/null
+++ b/onnxruntime/core/providers/tidl/tidl_provider_factory.cc
@@ -0,0 +1,47 @@
+// Copyright (c) Microsoft Corporation. All rights reserved.
+// Licensed under the MIT License.
+
+
+#include "core/providers/tidl/tidl_provider_factory.h"
+#include "tidl_execution_provider.h"
+#include "core/session/abi_session_options_impl.h"
+
+using namespace onnxruntime;
+
+namespace onnxruntime {
+
+struct TidlProviderFactory : IExecutionProviderFactory {
+  TidlProviderFactory(const std::string& type, const TIDLProviderOptions& options_tidl_onnx_vec)
+      : options_tidl_onnx_vec_(options_tidl_onnx_vec), type_(type) {}
+  ~TidlProviderFactory() override {}
+
+  std::unique_ptr<IExecutionProvider> CreateProvider() override;
+
+  private:
+  TIDLProviderOptions options_tidl_onnx_vec_;
+  std::string type_;
+};
+
+std::unique_ptr<IExecutionProvider> TidlProviderFactory::CreateProvider() {
+  //return onnxruntime::make_unique<TidlExecutionProvider>();
+  TidlExecutionProviderInfo info(type_, options_tidl_onnx_vec_);
+  return onnxruntime::make_unique<TidlExecutionProvider>(info);
+}
+
+std::shared_ptr<IExecutionProviderFactory> CreateExecutionProviderFactory_Tidl(const std::string &type, const TIDLProviderOptions& options_tidl_onnx_vec) {
+  return std::make_shared<onnxruntime::TidlProviderFactory>(type, options_tidl_onnx_vec);
+}
+}  // namespace onnxruntime
+
+ORT_API_STATUS_IMPL(OrtSessionOptionsAppendExecutionProvider_Tidl, _In_ OrtSessionOptions* options, c_api_tidl_options * options_tidl_onnx) {
+  TIDLProviderOptions options_tidl_onnx_vec;
+  
+  options_tidl_onnx_vec.push_back(std::make_pair("import", std::string(options_tidl_onnx->import)));
+  options_tidl_onnx_vec.push_back(std::make_pair("debug_level", std::to_string(options_tidl_onnx->debug_level)));
+  options_tidl_onnx_vec.push_back(std::make_pair("tidl_tensor_bits", std::to_string(options_tidl_onnx->tidl_tensor_bits)));
+  options_tidl_onnx_vec.push_back(std::make_pair("tidl_tools_path", std::string(options_tidl_onnx->tidl_tools_path)));
+  options_tidl_onnx_vec.push_back(std::make_pair("artifacts_folder", std::string(options_tidl_onnx->artifacts_folder)));
+
+  options->provider_factories.push_back(onnxruntime::CreateExecutionProviderFactory_Tidl("", options_tidl_onnx_vec));
+  return nullptr;
+}
diff --git a/onnxruntime/core/session/inference_session.cc b/onnxruntime/core/session/inference_session.cc
index 085293061..ce074ad72 100644
--- a/onnxruntime/core/session/inference_session.cc
+++ b/onnxruntime/core/session/inference_session.cc
@@ -55,13 +55,24 @@
 #include "core/framework/customregistry.h"
 #include "core/session/custom_ops.h"
 #endif
-
+#ifdef USE_TIDL  
+#include "core/providers/tidl/tidl_execution_provider.h"
+#include "core/framework/func_kernel.h"
+#endif
 using namespace ONNX_NAMESPACE;
 using namespace onnxruntime::experimental;
 using namespace onnxruntime::common;
 
 namespace onnxruntime {
 namespace {
+
+static inline void get_time_u64(uint64_t *t)
+{
+    struct timespec ts;
+    clock_gettime(CLOCK_MONOTONIC, &ts);
+    *t = (uint64_t)ts.tv_sec * (uint64_t)1000000000ull + (uint64_t)ts.tv_nsec;
+}
+
 template <typename T>
 const T* GetDateFormatString();
 
@@ -1506,6 +1517,19 @@ Status InferenceSession::Run(const RunOptions& run_options,
   if (session_profiler_.IsEnabled()) {
     tp = session_profiler_.StartTime();
   }
+  // For TIDL
+  auto tidl_ep = execution_providers_.Get(kTidlExecutionProvider);
+  if(tidl_ep)
+  {
+    const TidlExecutionProvider* tidl_ep_ = reinterpret_cast<const TidlExecutionProvider*>(tidl_ep);
+    tidl_ep_->GetCustomMemStats(&run_start_ddr_read, &run_start_ddr_write);
+  }
+  else
+  {
+    run_start_ddr_read = 0; run_start_ddr_write = 0;
+  }
+
+  get_time_u64(&run_start_ts);
 
 #ifdef ONNXRUNTIME_ENABLE_INSTRUMENT
   TraceLoggingActivity<telemetry_provider_handle> ortrun_activity;
@@ -1580,11 +1604,6 @@ Status InferenceSession::Run(const RunOptions& run_options,
                                                  session_options_.execution_mode, run_options.terminate, run_logger,
                                                  run_options.only_execute_path_to_fetches));
   }
-  ORT_CATCH(const std::exception& e) {
-    ORT_HANDLE_EXCEPTION([&]() {
-      retval = Status(common::ONNXRUNTIME, common::FAIL, e.what());
-    });
-  }
   ORT_CATCH(...) {
     retval = Status(common::ONNXRUNTIME, common::RUNTIME_EXCEPTION, "Encountered unknown exception in Run()");
   }
@@ -1622,6 +1641,19 @@ Status InferenceSession::Run(const RunOptions& run_options,
 #ifdef ONNXRUNTIME_ENABLE_INSTRUMENT
   TraceLoggingWriteStop(ortrun_activity, "OrtRun");
 #endif
+  
+  get_time_u64(&run_end_ts);
+  
+  if(tidl_ep)
+  {
+    const TidlExecutionProvider* tidl_ep_ = reinterpret_cast<const TidlExecutionProvider*>(tidl_ep);
+    tidl_ep_->GetCustomMemStats(&run_end_ddr_read, &run_end_ddr_write);
+  }
+  else
+  {
+    run_end_ddr_read = 0; run_end_ddr_write = 0;
+  }
+
   return retval;
 }
 
@@ -1946,5 +1978,57 @@ InferenceSession* SessionIOBinding::GetInferenceSession() {
 IOBinding* SessionIOBinding::Get() {
   return binding_.get();
 }
+std::vector<std::pair<std::string, uint64_t>> InferenceSession::get_TI_benchmark_data(){
+  std::vector<std::pair<std::string, uint64_t>> res;
+  std::vector<std::pair<std::string, void *>> c_data;
+
+  /* get the run duration */
+  res.push_back(std::make_pair<std::string, uint64_t>("ts:run_start", uint64_t(run_start_ts)));
+  res.push_back(std::make_pair<std::string, uint64_t>("ts:run_end", uint64_t(run_end_ts)));
+  /* get the ddr bw numbers */
+  res.push_back(std::make_pair<std::string, uint64_t>("ddr:read_start", uint64_t(run_start_ddr_read)));
+  res.push_back(std::make_pair<std::string, uint64_t>("ddr:read_end", uint64_t(run_end_ddr_read)));
+  res.push_back(std::make_pair<std::string, uint64_t>("ddr:write_start", uint64_t(run_start_ddr_write)));
+  res.push_back(std::make_pair<std::string, uint64_t>("ddr:write_end", uint64_t(run_end_ddr_write)));
+  char *node_name;
+  void *node_data;
+  const SequentialExecutionPlan& seq_exec_plan = *session_state_->GetExecutionPlan();
+  const auto& exec_plan_vec = seq_exec_plan.execution_plan;
+  const auto& graph_viewer= session_state_->GetGraphViewer();
+  Status status;
+
+    for (const auto& node_exec_plan : exec_plan_vec) 
+    {
+      auto node_index = node_exec_plan.node_index;
+      const auto& node = graph_viewer.GetNode(node_index);
+      if(node->OpType() == "TIDL_0")
+      {
+        auto p_op_kernel = session_state_->GetKernel(node_index);
+        const FunctionKernel * fun_op_kernel =  reinterpret_cast<const FunctionKernel*>(p_op_kernel);
+        status = fun_op_kernel->Custom(&node_name, &node_data);
+        if(status.IsOK())
+        {
+          c_data.push_back(std::make_pair(std::string(node_name), node_data));
+        }
+      }
+    }
+
+  if(c_data.size()) {
+      for (auto e : c_data) {
+          std::string prefix = "ts:subgraph_" + e.first + "_";
+          std::string annots[] = {
+              "copy_in_start", "copy_in_end",
+              "proc_start", "proc_end",
+              "copy_out_start", "copy_out_end"
+          };
+          std::vector<uint64_t> *s = static_cast<std::vector<uint64_t>*>(e.second);
+          int index = 0;
+          for(auto it = s->begin(); it != s->end(); it++, index++)
+              res.push_back(std::make_pair<std::string, uint64_t>(prefix + annots[index], uint64_t(*it)));
+          delete s;
+      }
+  }
+  return res;
+}
 
 }  // namespace onnxruntime
diff --git a/onnxruntime/core/session/inference_session.h b/onnxruntime/core/session/inference_session.h
index d81fa4961..b617423d4 100644
--- a/onnxruntime/core/session/inference_session.h
+++ b/onnxruntime/core/session/inference_session.h
@@ -396,6 +396,11 @@ class InferenceSession {
     *Get InferenceSession logger.
     */
   const logging::Logger* GetLogger() const { return session_logger_; };
+  
+  /** 
+   * Get subgraph level data for TIDL 
+   */
+  std::vector<std::pair<std::string, uint64_t>> get_TI_benchmark_data();
 
  protected:
 #if !defined(ORT_MINIMAL_BUILD)
@@ -662,6 +667,9 @@ class InferenceSession {
   std::vector<uint8_t> ort_format_model_bytes_;
 
   std::shared_ptr<onnxruntime::AllocatorManager> allocator_manager_;
+  uint64_t run_start_ts, run_start_ddr_read, run_start_ddr_write;
+  uint64_t run_end_ts, run_end_ddr_read, run_end_ddr_write;
+
 };
 
 struct SessionIOBinding {
diff --git a/onnxruntime/python/onnxruntime_inference_collection.py b/onnxruntime/python/onnxruntime_inference_collection.py
index e1603fcbe..f03cb6d84 100644
--- a/onnxruntime/python/onnxruntime_inference_collection.py
+++ b/onnxruntime/python/onnxruntime_inference_collection.py
@@ -196,7 +196,10 @@ class Session:
                 return self._sess.run(output_names, input_feed, run_options)
             else:
                 raise
-
+    
+    def get_TI_benchmark_data(self):
+        return self._sess.get_TI_benchmark_data()
+      
     def end_profiling(self):
         """
         End profiling and return results in a file.
@@ -338,6 +341,7 @@ class InferenceSession(Session):
         self._sess = None
         self._sess_options = self._sess_options_initial
         self._create_inference_session(providers, provider_options)
+    
 
 
 class IOBinding:
@@ -522,3 +526,4 @@ class OrtValue:
         Valid only for OrtValues holding Tensors. Throws for OrtValues holding non-Tensors.
         '''
         return self._ortvalue.numpy()
+    
diff --git a/onnxruntime/python/onnxruntime_pybind_state.cc b/onnxruntime/python/onnxruntime_pybind_state.cc
index 32bd9bc15..6f233263d 100644
--- a/onnxruntime/python/onnxruntime_pybind_state.cc
+++ b/onnxruntime/python/onnxruntime_pybind_state.cc
@@ -64,6 +64,15 @@ struct OrtStatus {
 #define BACKEND_MIGRAPHX ""
 #endif
 
+#if USE_TIDL
+#define BACKEND_TIDL "-TIDL"
+//#include "core/providers/tidl/tidl_execution_provider.h"
+#else
+#define BACKEND_TIDL ""
+#endif
+
+
+
 #ifdef USE_OPENVINO
 #if OPENVINO_CONFIG_CPU_FP32
 #define BACKEND_OPENVINO "-OPENVINO_CPU_FP32"
@@ -130,7 +139,7 @@ struct OrtStatus {
 #define BACKEND_DML ""
 #endif
 
-#define BACKEND_DEVICE BACKEND_PROC BACKEND_DNNL BACKEND_OPENVINO BACKEND_NUPHAR BACKEND_OPENBLAS BACKEND_MIGRAPHX BACKEND_ACL BACKEND_ARMNN BACKEND_DML
+#define BACKEND_DEVICE BACKEND_PROC BACKEND_DNNL BACKEND_TIDL BACKEND_OPENVINO BACKEND_NUPHAR BACKEND_OPENBLAS BACKEND_MIGRAPHX BACKEND_ACL BACKEND_ARMNN BACKEND_DML
 #include "core/session/onnxruntime_cxx_api.h"
 #include "core/providers/providers.h"
 #include "core/providers/cpu/cpu_execution_provider.h"
@@ -160,6 +169,11 @@ onnxruntime::ArenaExtendStrategy arena_extend_strategy = onnxruntime::ArenaExten
 #ifdef USE_MIGRAPHX
 #include "core/providers/migraphx/migraphx_provider_factory.h"
 #endif
+#ifdef USE_TIDL
+using TIDLProviderOptions = std::vector<std::pair<std::string,std::string>>;
+#include "core/providers/tidl/tidl_provider_factory.h"
+#endif
+
 #ifdef USE_OPENVINO
 #include "core/providers/openvino/openvino_provider_factory.h"
 // TODO remove deprecated global config
@@ -193,6 +207,7 @@ namespace onnxruntime {
 std::shared_ptr<IExecutionProviderFactory> CreateExecutionProviderFactory_Tensorrt(const OrtTensorRTProviderOptions* params);
 std::shared_ptr<IExecutionProviderFactory> CreateExecutionProviderFactory_MIGraphX(int device_id);
 std::shared_ptr<IExecutionProviderFactory> CreateExecutionProviderFactory_Dnnl(int use_arena);
+std::shared_ptr<IExecutionProviderFactory> CreateExecutionProviderFactory_Tidl(const std::string& provider_type, const TIDLProviderOptions& options);
 std::shared_ptr<IExecutionProviderFactory> CreateExecutionProviderFactory_OpenVINO(const OrtOpenVINOProviderOptions* params);
 #ifdef USE_OPENVINO
 const ProviderInfo_OpenVINO* GetProviderInfo_OpenVINO();
@@ -543,7 +558,20 @@ static void RegisterExecutionProviders(InferenceSession* sess, const std::vector
       RegisterExecutionProvider(
           sess, *onnxruntime::CreateExecutionProviderFactory_Dnnl(sess->GetSessionOptions().enable_cpu_mem_arena));
 #endif
-    } else if (type == kOpenVINOExecutionProvider) {
+    } 
+    else if (type == kTidlExecutionProvider || type == kTidlCompilationProvider) {
+#ifdef USE_TIDL
+      const auto it = provider_options_map.find(type);
+      TIDLProviderOptions tidl_options;
+      if(it != provider_options_map.end()) {
+        auto i_options = it->second;
+        for (auto elem = i_options.begin(); elem != i_options.end(); elem++)
+          tidl_options.push_back(std::make_pair(elem->first, elem->second));
+      }
+      RegisterExecutionProvider(sess, *onnxruntime::CreateExecutionProviderFactory_Tidl(type, tidl_options));
+#endif
+    }
+    else if (type == kOpenVINOExecutionProvider) {
 #ifdef USE_OPENVINO
       OrtOpenVINOProviderOptions params;
       params.device_type = openvino_device_type.c_str();
@@ -837,6 +865,10 @@ void addGlobalMethods(py::module& m, Environment& env) {
 #ifdef USE_DNNL
             onnxruntime::CreateExecutionProviderFactory_Dnnl(1),
 #endif
+#ifdef USE_TIDL
+            onnxruntime::CreateExecutionProviderFactory_Tidl(),
+#endif
+
 #ifdef USE_OPENVINO
             onnxruntime::CreateExecutionProviderFactory_OpenVINO(openvino_device_type, false, "", 8),
 #endif
@@ -1742,6 +1774,16 @@ including arg name, arg type (contains both type and shape).)pbdoc")
              }
              return rfetch;
            })
+      
+      .def("get_TI_benchmark_data", [](PyInferenceSession* sess) -> py::dict {
+        std::vector<std::pair<std::string, uint64_t>> res =  sess->GetSessionHandle()->get_TI_benchmark_data();
+        py::dict benchmark_dict;
+        for (auto e : res)
+        {
+            benchmark_dict[e.first.c_str()] = e.second;
+        }
+        return benchmark_dict;
+      })
       .def("end_profiling", [](PyInferenceSession* sess) -> std::string {
         return sess->GetSessionHandle()->EndProfiling();
       })
diff --git a/onnxruntime/test/util/include/providers.h b/onnxruntime/test/util/include/providers.h
index 2f6a62b9a..42db07e17 100644
--- a/onnxruntime/test/util/include/providers.h
+++ b/onnxruntime/test/util/include/providers.h
@@ -10,6 +10,9 @@
 #ifdef USE_DNNL
 #include "core/providers/dnnl/dnnl_provider_factory.h"
 #endif
+#ifdef USE_TIDL
+#include "core/providers/tidl/tidl_provider_factory.h"
+#endif
 #ifdef USE_NUPHAR
 #include "core/providers/nuphar/nuphar_provider_factory.h"
 #endif
diff --git a/setup.py b/setup.py
index 9e269ad2d..7cd86d7fa 100644
--- a/setup.py
+++ b/setup.py
@@ -41,6 +41,9 @@ for arg in sys.argv[1:]:
 if '--use_tensorrt' in sys.argv:
     package_name = 'onnxruntime-gpu-tensorrt' if not nightly_build else 'ort-trt-nightly'
     sys.argv.remove('--use_tensorrt')
+if '--use_tidl' in sys.argv:
+    package_name = 'onnxruntime-tidl'
+    sys.argv.remove('--use_tidl')
 elif '--use_cuda' in sys.argv:
     package_name = 'onnxruntime-gpu' if not nightly_build else 'ort-gpu-nightly'
     sys.argv.remove('--use_cuda')
diff --git a/tidl_demos/README b/tidl_demos/README
new file mode 100644
index 000000000..e2b350973
--- /dev/null
+++ b/tidl_demos/README
@@ -0,0 +1,17 @@
+# TIDL Integration to ONNX Runtime
+
+## Execution Provider
+The core of the execution proivder can be found [HERE](https://bitbucket.itg.ti.com/users/a0226847/repos/onnxruntime/browse/onnxruntime/core/providers/tidl). Checkout [THIS REPO DIFF](https://bitbucket.itg.ti.com/users/a0226847/repos/onnxruntime/compare/diff?targetBranch=refs%2Fheads%2Fmaster&sourceBranch=refs%2Fheads%2Ftidl_xp&targetRepoId=33757) for a full description of what files need to be changed to add TIDL support to ONNX Runtime.
+
+This execution provider is heavily based on the DNNL execution provider, which relies on Intel's library which gets built and linked from here: `ORT_HOME/build/Linux/RelWithDebInfo/dnnl/install`. This gets cloned/built during the long and complicated Cmake build process, I would imagine something similar will need to be added for TIDL libs.
+
+## Building ONNX Runtime
+You should be able to just run ./tidl_build.sh in this directory. Cmake must be installed.
+
+## Demos
+
+### Basic Demo
+[Basic Demo documentation](https://bitbucket.itg.ti.com/users/a0226847/repos/onnxruntime/browse/tidl_demos/basic_demo)
+
+### Image Classification Demo
+[Image Classification Demo documentation](https://bitbucket.itg.ti.com/users/a0226847/repos/onnxruntime/browse/tidl_demos/classification_demo)
\ No newline at end of file
diff --git a/tidl_demos/basic_demo/CXX_Api_Sample.cpp b/tidl_demos/basic_demo/CXX_Api_Sample.cpp
new file mode 100644
index 000000000..26885f6f6
--- /dev/null
+++ b/tidl_demos/basic_demo/CXX_Api_Sample.cpp
@@ -0,0 +1,138 @@
+// Copyright(c) Microsoft Corporation.All rights reserved.
+// Licensed under the MIT License.
+//
+
+#include <assert.h>
+#include <vector>
+#include <onnxruntime/core/session/onnxruntime_cxx_api.h>
+#include <onnxruntime/core/providers/tidl/tidl_provider_factory.h>
+#include <onnxruntime/core/providers/dnnl/dnnl_provider_factory.h>
+
+int main(int argc, char* argv[]) {
+  //*************************************************************************
+  // initialize  enviroment...one enviroment per process
+  // enviroment maintains thread pools and other state info
+  Ort::Env env(ORT_LOGGING_LEVEL_WARNING, "test");
+
+  // initialize session options if needed
+  Ort::SessionOptions session_options;
+  session_options.SetIntraOpNumThreads(1);
+
+  // If onnxruntime.dll is built with CUDA enabled, we can uncomment out this line to use CUDA for this
+  // session (we also need to include cuda_provider_factory.h above which defines it)
+  // #include "cuda_provider_factory.h"
+  // OrtSessionOptionsAppendExecutionProvider_Dnnl(session_options, 1);
+  OrtSessionOptionsAppendExecutionProvider_Tidl(session_options, 1);
+
+  // Sets graph optimization level
+  // Available levels are
+  // ORT_DISABLE_ALL -> To disable all optimizations
+  // ORT_ENABLE_BASIC -> To enable basic optimizations (Such as redundant node removals)
+  // ORT_ENABLE_EXTENDED -> To enable extended optimizations (Includes level 1 + more complex optimizations like node fusions)
+  // ORT_ENABLE_ALL -> To Enable All possible opitmizations
+  session_options.SetGraphOptimizationLevel(GraphOptimizationLevel::ORT_ENABLE_EXTENDED);
+
+  //*************************************************************************
+  // create session and load model into memory
+  // using squeezenet version 1.3
+  // URL = https://github.com/onnx/models/tree/master/squeezenet
+#ifdef _WIN32
+  const wchar_t* model_path = L"squeezenet.onnx";
+#else
+  const char* model_path = "../../csharp/testdata/squeezenet.onnx";
+  // const char* model_path = "../onnxruntime/python/tools/quantization/E2E_example_model/resnet50_v1.onnx";
+  // const char* model_path = "../build/Linux/RelWithDebInfo/2-layer-nested-subgraph-test.onnx";
+#endif
+
+  printf("Using Onnxruntime C++ API\n");
+  Ort::Session session(env, model_path, session_options);
+
+  //*************************************************************************
+  // print model input layer (node names, types, shape etc.)
+  Ort::AllocatorWithDefaultOptions allocator;
+
+  // print number of model input nodes
+  size_t num_input_nodes = session.GetInputCount();
+  std::vector<const char*> input_node_names(num_input_nodes);
+  std::vector<int64_t> input_node_dims;  // simplify... this model has only 1 input node {1, 3, 224, 224}.
+                                         // Otherwise need vector<vector<>>
+
+  printf("Number of inputs = %zu\n", num_input_nodes);
+
+  // iterate over all input nodes
+  for (int i = 0; i < num_input_nodes; i++) {
+    // print input node names
+    char* input_name = session.GetInputName(i, allocator);
+    printf("Input %d : name=%s\n", i, input_name);
+    input_node_names[i] = input_name;
+
+    // print input node types
+    Ort::TypeInfo type_info = session.GetInputTypeInfo(i);
+    auto tensor_info = type_info.GetTensorTypeAndShapeInfo();
+
+    ONNXTensorElementDataType type = tensor_info.GetElementType();
+    printf("Input %d : type=%d\n", i, type);
+
+    // print input shapes/dims
+    input_node_dims = tensor_info.GetShape();
+    printf("Input %d : num_dims=%zu\n", i, input_node_dims.size());
+    for (int j = 0; j < input_node_dims.size(); j++)
+      printf("Input %d : dim %d=%jd\n", i, j, input_node_dims[j]);
+  }
+
+  // Results should be...
+  // Number of inputs = 1
+  // Input 0 : name = data_0
+  // Input 0 : type = 1
+  // Input 0 : num_dims = 4
+  // Input 0 : dim 0 = 1
+  // Input 0 : dim 1 = 3
+  // Input 0 : dim 2 = 224
+  // Input 0 : dim 3 = 224
+
+  //*************************************************************************
+  // Similar operations to get output node information.
+  // Use OrtSessionGetOutputCount(), OrtSessionGetOutputName()
+  // OrtSessionGetOutputTypeInfo() as shown above.
+
+  //*************************************************************************
+  // Score the model using sample data, and inspect values
+
+  size_t input_tensor_size = 224 * 224 * 3;  // simplify ... using known dim values to calculate size
+                                             // use OrtGetTensorShapeElementCount() to get official size!
+
+  std::vector<float> input_tensor_values(input_tensor_size);
+  std::vector<const char*> output_node_names = {"softmaxout_1"};
+
+  // initialize input data with values in [0.0, 1.0]
+  for (unsigned int i = 0; i < input_tensor_size; i++)
+    input_tensor_values[i] = (float)i / (input_tensor_size + 1);
+
+  // create input tensor object from data values
+  auto memory_info = Ort::MemoryInfo::CreateCpu(OrtArenaAllocator, OrtMemTypeDefault);
+  Ort::Value input_tensor = Ort::Value::CreateTensor<float>(memory_info, input_tensor_values.data(), input_tensor_size, input_node_dims.data(), 4);
+  assert(input_tensor.IsTensor());
+
+  // score model & input tensor, get back output tensor
+  auto run_options = Ort::RunOptions();
+  run_options.SetRunLogVerbosityLevel(2);
+  auto output_tensors = session.Run(run_options, input_node_names.data(), &input_tensor, 1, output_node_names.data(), 1);
+  assert(output_tensors.size() == 1 && output_tensors.front().IsTensor());
+
+  // Get pointer to output tensor float values
+  float* floatarr = output_tensors.front().GetTensorMutableData<float>();
+  assert(abs(floatarr[0] - 0.000045) < 1e-6);
+
+  // score the model, and print scores for first 5 classes
+  for (int i = 0; i < 5; i++)
+    printf("Score for class [%d] =  %f\n", i, floatarr[i]);
+
+  // Results should be as below...
+  // Score for class[0] = 0.000045
+  // Score for class[1] = 0.003846
+  // Score for class[2] = 0.000125
+  // Score for class[3] = 0.001180
+  // Score for class[4] = 0.001317
+  printf("Done!\n");
+  return 0;
+}
diff --git a/tidl_demos/basic_demo/C_Api_Sample.cpp b/tidl_demos/basic_demo/C_Api_Sample.cpp
new file mode 100644
index 000000000..93aec5d0c
--- /dev/null
+++ b/tidl_demos/basic_demo/C_Api_Sample.cpp
@@ -0,0 +1,170 @@
+// Copyright(c) Microsoft Corporation.All rights reserved.
+// Licensed under the MIT License.
+//
+
+#include <assert.h>
+#include <onnxruntime_c_api.h>
+#include <cmath>
+#include <stdlib.h>
+#include <stdio.h>
+#include <vector>
+
+const OrtApi* g_ort = OrtGetApiBase()->GetApi(ORT_API_VERSION);
+
+//*****************************************************************************
+// helper function to check for status
+void CheckStatus(OrtStatus* status)
+{
+    if (status != NULL) {
+      const char* msg = g_ort->GetErrorMessage(status);
+      fprintf(stderr, "%s\n", msg);
+      g_ort->ReleaseStatus(status);
+      exit(1);
+    }
+}
+
+int main(int argc, char* argv[]) {
+  //*************************************************************************
+  // initialize  enviroment...one enviroment per process
+  // enviroment maintains thread pools and other state info
+  OrtEnv* env;
+  CheckStatus(g_ort->CreateEnv(ORT_LOGGING_LEVEL_WARNING, "test", &env));
+
+  // initialize session options if needed
+  OrtSessionOptions* session_options;
+  CheckStatus(g_ort->CreateSessionOptions(&session_options));
+  g_ort->SetIntraOpNumThreads(session_options, 1);
+
+  // Sets graph optimization level
+  g_ort->SetSessionGraphOptimizationLevel(session_options, ORT_ENABLE_BASIC);
+
+  // Optionally add more execution providers via session_options
+  // E.g. for CUDA include cuda_provider_factory.h and uncomment the following line:
+  // OrtSessionOptionsAppendExecutionProvider_CUDA(sessionOptions, 0);
+
+  //*************************************************************************
+  // create session and load model into memory
+  // using squeezenet version 1.3
+  // URL = https://github.com/onnx/models/tree/master/squeezenet
+  OrtSession* session;
+#ifdef _WIN32
+  const wchar_t* model_path = L"squeezenet.onnx";
+#else
+  const char* model_path = "squeezenet.onnx";
+#endif
+
+  printf("Using Onnxruntime C API\n");
+  CheckStatus(g_ort->CreateSession(env, model_path, session_options, &session));
+
+  //*************************************************************************
+  // print model input layer (node names, types, shape etc.)
+  size_t num_input_nodes;
+  OrtStatus* status;
+  OrtAllocator* allocator;
+  CheckStatus(g_ort->GetAllocatorWithDefaultOptions(&allocator));
+
+  // print number of model input nodes
+  status = g_ort->SessionGetInputCount(session, &num_input_nodes);
+  std::vector<const char*> input_node_names(num_input_nodes);
+  std::vector<int64_t> input_node_dims;  // simplify... this model has only 1 input node {1, 3, 224, 224}.
+                                         // Otherwise need vector<vector<>>
+
+  printf("Number of inputs = %zu\n", num_input_nodes);
+
+  // iterate over all input nodes
+  for (size_t i = 0; i < num_input_nodes; i++) {
+    // print input node names
+    char* input_name;
+    status = g_ort->SessionGetInputName(session, i, allocator, &input_name);
+    printf("Input %zu : name=%s\n", i, input_name);
+    input_node_names[i] = input_name;
+
+    // print input node types
+    OrtTypeInfo* typeinfo;
+    status = g_ort->SessionGetInputTypeInfo(session, i, &typeinfo);
+    const OrtTensorTypeAndShapeInfo* tensor_info;
+	CheckStatus(g_ort->CastTypeInfoToTensorInfo(typeinfo, &tensor_info));
+    ONNXTensorElementDataType type;
+	CheckStatus(g_ort->GetTensorElementType(tensor_info, &type));
+    printf("Input %zu : type=%d\n", i, type);
+
+    // print input shapes/dims
+    size_t num_dims;
+	CheckStatus(g_ort->GetDimensionsCount(tensor_info, &num_dims));
+    printf("Input %zu : num_dims=%zu\n", i, num_dims);
+    input_node_dims.resize(num_dims);
+	g_ort->GetDimensions(tensor_info, (int64_t*)input_node_dims.data(), num_dims);
+    for (size_t j = 0; j < num_dims; j++)
+      printf("Input %zu : dim %zu=%jd\n", i, j, input_node_dims[j]);
+
+	g_ort->ReleaseTypeInfo(typeinfo);
+  }
+
+  // Results should be...
+  // Number of inputs = 1
+  // Input 0 : name = data_0
+  // Input 0 : type = 1
+  // Input 0 : num_dims = 4
+  // Input 0 : dim 0 = 1
+  // Input 0 : dim 1 = 3
+  // Input 0 : dim 2 = 224
+  // Input 0 : dim 3 = 224
+
+  //*************************************************************************
+  // Similar operations to get output node information.
+  // Use OrtSessionGetOutputCount(), OrtSessionGetOutputName()
+  // OrtSessionGetOutputTypeInfo() as shown above.
+
+  //*************************************************************************
+  // Score the model using sample data, and inspect values
+
+  size_t input_tensor_size = 224 * 224 * 3;  // simplify ... using known dim values to calculate size
+                                             // use OrtGetTensorShapeElementCount() to get official size!
+
+  std::vector<float> input_tensor_values(input_tensor_size);
+  std::vector<const char*> output_node_names = {"softmaxout_1"};
+
+  // initialize input data with values in [0.0, 1.0]
+  for (size_t i = 0; i < input_tensor_size; i++)
+    input_tensor_values[i] = (float)i / (input_tensor_size + 1);
+
+  // create input tensor object from data values
+  OrtMemoryInfo* memory_info;
+  CheckStatus(g_ort->CreateCpuMemoryInfo(OrtArenaAllocator, OrtMemTypeDefault, &memory_info));
+  OrtValue* input_tensor = NULL;
+  CheckStatus(g_ort->CreateTensorWithDataAsOrtValue(memory_info, input_tensor_values.data(), input_tensor_size * sizeof(float), input_node_dims.data(), 4, ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT, &input_tensor));
+  int is_tensor;
+  CheckStatus(g_ort->IsTensor(input_tensor, &is_tensor));
+  assert(is_tensor);
+  g_ort->ReleaseMemoryInfo(memory_info);
+
+  // score model & input tensor, get back output tensor
+  OrtValue* output_tensor = NULL;
+  CheckStatus(g_ort->Run(session, NULL, input_node_names.data(), (const OrtValue* const*)&input_tensor, 1, output_node_names.data(), 1, &output_tensor));
+  CheckStatus(g_ort->IsTensor(output_tensor, &is_tensor));
+  assert(is_tensor);
+
+  // Get pointer to output tensor float values
+  float* floatarr;
+  CheckStatus(g_ort->GetTensorMutableData(output_tensor, (void**)&floatarr));
+  assert(std::abs(floatarr[0] - 0.000045) < 1e-6);
+
+  // score the model, and print scores for first 5 classes
+  for (int i = 0; i < 5; i++)
+    printf("Score for class [%d] =  %f\n", i, floatarr[i]);
+
+  // Results should be as below...
+  // Score for class[0] = 0.000045
+  // Score for class[1] = 0.003846
+  // Score for class[2] = 0.000125
+  // Score for class[3] = 0.001180
+  // Score for class[4] = 0.001317
+
+  g_ort->ReleaseValue(output_tensor);
+  g_ort->ReleaseValue(input_tensor);
+  g_ort->ReleaseSession(session);
+  g_ort->ReleaseSessionOptions(session_options);
+  g_ort->ReleaseEnv(env);
+  printf("Done!\n");
+  return 0;
+}
diff --git a/tidl_demos/basic_demo/InferenceTestCapi.cpp b/tidl_demos/basic_demo/InferenceTestCapi.cpp
new file mode 100644
index 000000000..f5b43d554
--- /dev/null
+++ b/tidl_demos/basic_demo/InferenceTestCapi.cpp
@@ -0,0 +1,91 @@
+// Copyright (c) Microsoft Corporation. All rights reserved.
+// Licensed under the MIT License.
+//
+#include "CppUnitTest.h"
+#include <assert.h>
+#include <onnxruntime_c_api.h>
+
+wchar_t* GetWideString(const char* c) {
+  const size_t cSize = strlen(c) + 1;
+  wchar_t* wc = new wchar_t[cSize];
+  mbstowcs(wc, c, cSize);
+
+  return wc;
+}
+
+#define ORT_ABORT_ON_ERROR(expr)				\
+  {								\
+    OrtStatus* onnx_status = (expr);				\
+    if (onnx_status != NULL) {					\
+      const char* msg = OrtGetErrorMessage(onnx_status);	\
+      fprintf(stderr, "%s\n", msg);				\
+      OrtReleaseStatus(onnx_status);				\
+      wchar_t* wmsg = GetWideString(msg);			\
+      Assert::Fail(L"Failed on ORT_ABORT_ON_ERROR");		\
+      free(wmsg);						\
+    }								\
+  }
+
+using namespace Microsoft::VisualStudio::CppUnitTestFramework;
+
+namespace UnitTest1 {
+  TEST_CLASS(UnitTest1){
+  public :
+
+    int run_inference(OrtSession * session){
+      size_t input_height = 224;
+      size_t input_width = 224;
+      float* model_input = (float*)malloc(sizeof(float) * 224 * 224 * 3);
+      size_t model_input_ele_count = 224 * 224 * 3;
+
+      // initialize to values between 0.0 and 1.0
+      for (unsigned int i = 0; i < model_input_ele_count; i++)
+	model_input[i] = (float)i / (float)(model_input_ele_count + 1);
+
+      OrtMemoryInfo* memory_info;
+      ORT_ABORT_ON_ERROR(OrtCreateCpuMemoryInfo(OrtArenaAllocator, OrtMemTypeDefault, &memory_info));
+      const size_t input_shape[] = {1, 3, 224, 224};
+      const size_t input_shape_len = sizeof(input_shape) / sizeof(input_shape[0]);
+      const size_t model_input_len = model_input_ele_count * sizeof(float);
+
+      OrtValue* input_tensor = NULL;
+      ORT_ABORT_ON_ERROR(OrtCreateTensorWithDataAsOrtValue(memory_info, model_input, model_input_len, input_shape, input_shape_len, ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT, &input_tensor));
+      assert(input_tensor != NULL);
+      assert(OrtIsTensor(input_tensor));
+      OrtReleaseMemoryInfo(memory_info);
+      const char* input_names[] = {"data_0"};
+      const char* output_names[] = {"softmaxout_1"};
+      OrtValue* output_tensor = NULL;
+      ORT_ABORT_ON_ERROR(OrtRun(session, NULL, input_names, (const OrtValue* const*)&input_tensor, 1, output_names, 1, &output_tensor));
+      assert(output_tensor != NULL);
+      assert(OrtIsTensor(output_tensor));
+
+      OrtReleaseValue(output_tensor);
+      OrtReleaseValue(input_tensor);
+      free(model_input);
+      return 0;
+    }  // namespace UnitTest1
+
+    int test() {
+      const wchar_t* model_path = L"squeezenet.onnx";
+      OrtEnv* env;
+      ORT_ABORT_ON_ERROR(OrtCreateEnv(ORT_LOGGING_LEVEL_WARNING, "test", &env));
+      OrtSessionOptions* session_option = OrtCreateSessionOptions();
+      OrtSession* session;
+      OrtSetIntraOpNumThreads(session_option, 1);
+      ORT_ABORT_ON_ERROR(OrtCreateSession(env, model_path, session_option, &session));
+      OrtReleaseSessionOptions(session_option);
+
+      int result = run_inference(session);
+
+      OrtReleaseSession(session);
+      OrtReleaseEnv(env);
+    }
+
+    TEST_METHOD(TestMethod1) {
+      int res = test();
+      Assert::AreEqual(res, 0);
+    }
+  }
+  ;
+}
diff --git a/tidl_demos/basic_demo/README b/tidl_demos/basic_demo/README
new file mode 100644
index 000000000..384e6d99c
--- /dev/null
+++ b/tidl_demos/basic_demo/README
@@ -0,0 +1,10 @@
+# ONNX Runtime Basic Demo
+
+## Building
+```sh
+# Prepare the environment with libs etc
+source ./setenv.sh
+
+# Build the demo
+make
+```
\ No newline at end of file
diff --git a/tidl_demos/basic_demo/makefile b/tidl_demos/basic_demo/makefile
new file mode 100644
index 000000000..8a9d60b42
--- /dev/null
+++ b/tidl_demos/basic_demo/makefile
@@ -0,0 +1,31 @@
+################################################################################
+# Tests (name of cpp file will be name of exe)
+TESTS = CXX_Api_Sample C_Api_Sample
+
+################################################################################
+# Paths
+ORTROOT=../..
+
+################################################################################
+# Compiler Config
+CXX = g++
+CXXFLAGS = -std=c++14
+INC = -I$(ORTROOT)/include -I$(ORTROOT)/include/onnxruntime/core/session/
+CXXFLAGS += $(INC)
+
+################################################################################
+# Linker Config
+LIBDIR = $(ORTROOT)/build/Linux/RelWithDebInfo/
+LIBS = onnxruntime
+LDFLAGS = -L$(LIBDIR) -l$(LIBS)
+
+################################################################################
+# Targets
+all: $(TESTS)
+
+$(TESTS): %: %.cpp
+	$(CXX) -o $@ $^ $(CXXFLAGS) $(LDFLAGS)
+
+.PHONY: clean
+clean:
+	rm -rf $(TESTS)
diff --git a/tidl_demos/basic_demo/setenv.sh b/tidl_demos/basic_demo/setenv.sh
new file mode 100755
index 000000000..eb1ce1b49
--- /dev/null
+++ b/tidl_demos/basic_demo/setenv.sh
@@ -0,0 +1,12 @@
+#!/bin/bash
+
+################################################################################
+# Ensure script is only run sourced
+if [[ $_ == $0 ]]; then
+    echo "!!!ERROR!!! - PLEASE RUN WITH 'source'"
+    exit 1
+fi
+
+################################################################################
+# Set environment variables
+export LD_LIBRARY_PATH=../build/Linux/RelWithDebInfo/
diff --git a/tidl_demos/classification_demo/README b/tidl_demos/classification_demo/README
new file mode 100644
index 000000000..0ce322852
--- /dev/null
+++ b/tidl_demos/classification_demo/README
@@ -0,0 +1,25 @@
+# ONNX Runtime Classification Demo
+
+Basic image classification demo using ONNX runtime. Currently the only working execution
+provider is DNNL, and the only tested model is Squeezenet. Image size is hard-coded to
+224x224x3, layout is hardcoded to NCHW, and the only supported image format is .png.
+
+## Usage
+```
+Usage: classification_demo <image_path> <model_path> <labels_path> [-t] [-h]
+Options:
+    image_path	Path to the input image to classify
+    model_path	Path to the ONNX model
+    labels_path	Path to the labels txt file
+    -t		Use the TIDL execution provider (default DNNL)
+    -h		Display this help text
+```
+
+## Building
+```sh
+# Prepare the environment with libs etc
+source ./setenv.sh
+
+# Build the demo, bins will be in the ./bin directory
+make
+```
\ No newline at end of file
diff --git a/tidl_demos/classification_demo/main.cc b/tidl_demos/classification_demo/main.cc
new file mode 100644
index 000000000..e720a4943
--- /dev/null
+++ b/tidl_demos/classification_demo/main.cc
@@ -0,0 +1,164 @@
+#include <getopt.h>
+#include <iostream>
+#include <cstdarg>
+#include <cstdio>
+#include <fstream>
+
+#include <onnxruntime/core/session/onnxruntime_cxx_api.h>
+#include <onnxruntime/core/providers/tidl/tidl_provider_factory.h>
+#include <onnxruntime/core/providers/dnnl/dnnl_provider_factory.h>
+
+#include "validator.h"
+
+#include <opencv2/core.hpp>
+#include <opencv2/imgproc.hpp>
+#include <opencv2/highgui.hpp>
+#include <opencv2/videoio.hpp>
+
+bool SetupInput(std::string input_path, cv::Mat& input_image)
+{
+    // Read image input
+    input_image = cv::imread(input_path, CV_LOAD_IMAGE_COLOR);
+    if(! input_image.data )                              // Check for invalid input
+    {
+        std::cout <<  "Could not open or find the image" << std::endl;
+        return false;
+    }
+    std::cout << "Image input: " << input_path.c_str() << std::endl;
+    return true;
+}
+
+/*
+ * Retrieve frame, resize, and record in NCHW format
+ */
+void CollectFrames(std::vector<uint8_t> &output,
+                   cv::Mat &in_image,
+                   int width, int height, int channels)
+{
+    cv::Mat image;
+    cv::resize(in_image, image, cv::Size(width, height));
+    cv::Mat *spl = new cv::Mat[channels];
+    split(image,spl);
+    
+    // Read the frame in NCHW format
+    output.resize(height * width * channels);
+    int idx = 0;
+    for(int c = 0; c < channels; c++)
+    {
+        const unsigned char* data = image.ptr();
+        for(int x = 0; x < width; x++)
+        {
+            for(int y = 0; y < height; y++)
+            {
+                output[idx++] =
+                    (uint8_t)data[(channels) * (y + x*width) + (channels - 1) - c];
+            }
+        }
+    }
+}
+
+int main(int argc, char* argv[])
+{
+    std::string model_path = "";
+    std::string image_path = "";
+    std::string labels_path = "";
+    
+    int tidl_flag = 0;
+    int index;
+    int c;
+
+    opterr = 0;
+
+    const char* help_str =
+        "Usage: classification_demo <image_path> <model_path> <labels_path> [-t] [-h]\n"
+        "Options:\n"
+        "    image_path\tPath to the input image to classify\n"
+        "    model_path\tPath to the ONNX model\n"
+        "    labels_path\tPath to the labels txt file\n"
+        "    -t\t\tUse the TIDL execution provider (default DNNL)\n"
+        "    -h\t\tDisplay this help text"
+        "\n";
+
+    while ((c = getopt (argc, argv, "toh")) != -1)
+        switch (c)
+        {
+        case 't':
+            tidl_flag = 1;
+            break;
+        case 'h':
+            fprintf (stdout, help_str, optopt);
+            return 0;
+        case '?':
+            if (isprint (optopt))
+                fprintf (stderr, "Unknown option `-%c'.\n", optopt);
+            else
+                fprintf (stderr,
+                         "Unknown option character `\\x%x'.\n",
+                         optopt);
+            return 1;
+        default:
+            abort ();
+        }
+
+    if ((argc - optind) < 3) {
+        fprintf (stderr, help_str, optopt);
+        return 1;
+    }
+
+    std::cout << argc - optind << std::endl;
+
+    image_path = std::string(argv[optind]);
+    model_path = std::string(argv[optind+1]);
+    labels_path = std::string(argv[optind+2]);
+
+    std::cout << image_path << std::endl;
+    std::cout << model_path << std::endl;
+    std::cout << labels_path << std::endl;
+
+    for (index = optind + 2; index < argc; index++)
+    {
+        printf ("!!! Ignoring argument %s\n", argv[index]);
+    }
+
+    
+    OrtStatus *status;
+    
+    // Initialize  enviroment, maintains thread pools and state info
+    Ort::Env env(ORT_LOGGING_LEVEL_WARNING, "test");
+    
+    // Initialize session options
+    Ort::SessionOptions session_options;
+    session_options.SetIntraOpNumThreads(1);
+
+    c_api_tidl_options * options = (c_api_tidl_options *)malloc(sizeof(c_api_tidl_options));
+    strcpy(options->import, "no");
+    options->debug_level = 0;
+    options->tidl_tensor_bits = 8;
+    strcpy(options->tidl_tools_path, "../../../tidl/c7x-mma-tidl/tidl_tools/");
+    strcpy(options->artifacts_folder, "../../../onnxrt-artifacts/");
+
+    if (tidl_flag)
+    {
+        status = OrtSessionOptionsAppendExecutionProvider_Tidl(session_options, options);
+    } else
+    {
+        status = OrtSessionOptionsAppendExecutionProvider_Dnnl(session_options, 1);
+    }
+    
+    session_options.SetGraphOptimizationLevel(GraphOptimizationLevel::ORT_ENABLE_EXTENDED);
+
+    // Create Validator
+    cv::Mat input_image;
+    std::vector<uint8_t> image_data;
+
+    // Process the input image
+    SetupInput(image_path, input_image);
+    CollectFrames(image_data, input_image, 224, 224, 3);
+
+    // Do the thing
+    Validator validator(env, model_path, labels_path, session_options, image_data);
+
+    printf("Done!\n");
+    return 0;
+}
+
diff --git a/tidl_demos/classification_demo/makefile b/tidl_demos/classification_demo/makefile
new file mode 100644
index 000000000..2b19fb5ac
--- /dev/null
+++ b/tidl_demos/classification_demo/makefile
@@ -0,0 +1,61 @@
+################################################################################
+# Paths
+ORTROOT=../..
+
+################################################################################
+# Build directories
+OBJDIR = ./obj
+OUTDIR = ./bin
+
+################################################################################
+# Compiler Config
+CXX = g++
+CXXFLAGS = -std=c++14 -g
+INC  = -I$(ORTROOT)/include -I$(ORTROOT)/include/onnxruntime/core/session/ -I$(ORTROOT)/../../opencv/opencv-3.1.0/modules/core/include
+INC += -I$(ORTROOT)/../../opencv/opencv-3.1.0/modules/highgui/include
+INC += -I$(ORTROOT)/../../opencv/opencv-3.1.0/modules/imgcodecs/include
+INC += -I$(ORTROOT)/../../opencv/opencv-3.1.0/modules/videoio/include
+INC += -I$(ORTROOT)/../../opencv/opencv-3.1.0/modules/imgproc/include
+CXXFLAGS += $(INC) -pthread -g
+
+################################################################################
+# Linker Config
+LIBDIR = -L$(ORTROOT)/build/Linux/Debug/ -L$(ORTROOT)/../../opencv/opencv-3.1.0/cmake/lib -L$(ORTROOT)/../../opencv/opencv-3.1.0/cmake/3rdparty/lib
+LIBS = -lonnxruntime
+
+LIBS += -lopencv_imgcodecs
+LIBS += -lopencv_imgproc
+LIBS += -lopencv_core
+LIBS += -llibtiff 
+LIBS += -llibwebp
+LIBS += -llibpng
+LIBS += -llibjpeg
+LIBS += -lIlmImf
+LIBS += -lzlib
+LIBS += -llibjasper
+
+
+LDFLAGS = $(LIBDIR) $(LIBS) -ldl -pthread
+
+################################################################################
+# Srcs Etc
+SRCS := main.cc validator.cc
+OBJS := $(SRCS:%=$(OBJDIR)/%.o)
+
+################################################################################
+# Targets
+all: $(OBJDIR) $(OUTDIR) $(OUTDIR)/classification_demo
+
+$(OUTDIR)/classification_demo: $(OBJS) 
+	$(CXX)  $(CXXFLAGS) $(OBJS)  $(LDFLAGS)  -o $@ 
+
+# c++ source
+$(OBJDIR)/%.cc.o: %.cc
+	$(CXX) $(CPPFLAGS) $(CXXFLAGS) -c $< -o $@ 
+
+$(OBJDIR) $(OUTDIR):
+	mkdir -p $@
+
+.PHONY: clean
+clean:
+	rm -rf $(OBJDIR) $(OUTDIR)
diff --git a/tidl_demos/classification_demo/models.py b/tidl_demos/classification_demo/models.py
new file mode 100644
index 000000000..cf2d6e915
--- /dev/null
+++ b/tidl_demos/classification_demo/models.py
@@ -0,0 +1,207 @@
+import os
+import platform
+import numpy as np
+from PIL import Image, ImageFont, ImageDraw, ImageEnhance
+
+if platform.machine() == 'aarch64':
+    dataset_base = '/home/root'
+    numImages = 100
+else : 
+    dataset_base = '/home/anand/workarea/deps/'
+    numImages = 3
+
+tidl_tensor_bits = 8
+numFramesCalibration = 3
+biasCalibrationIterations = 2
+tidl_calibration_accuracy_level = 1
+num_tidl_subgraphs = 16
+debug_level = 0
+power_of_2_quantization = 'no'
+enable_high_resolution_optimization = 'no'
+pre_batchnorm_fold = 1
+
+tidl_tools_path = '/home/a0393754/work/onnxrt_7.3_rel/c7x-mma-tidl/tidl_tools/'
+
+artifacts_folder = '../../../onnxrt-artifacts/'
+
+required_options = {
+"tidl_tools_path":tidl_tools_path,
+"artifacts_folder":artifacts_folder,
+"import":'no'
+}
+optional_options = {
+"tidl_platform":"J7",
+"tidl_version":"7.2",
+"tidl_tensor_bits":tidl_tensor_bits,
+"debug_level":debug_level,
+"num_tidl_subgraphs":num_tidl_subgraphs,
+"tidl_denylist":"",
+"tidl_calibration_accuracy_level":tidl_calibration_accuracy_level,
+"tidl_calibration_options:num_frames_calibration": numFramesCalibration,
+"tidl_calibration_options:bias_calibration_iterations": biasCalibrationIterations,
+"power_of_2_quantization": power_of_2_quantization,
+"enable_high_resolution_optimization": enable_high_resolution_optimization,
+"pre_batchnorm_fold" : pre_batchnorm_fold,
+"reserved" : 1601
+}
+
+#lables = '../testvecs/input/labels.txt'
+lables = '/home/a0393754/work/onnxrt_7.3_rel/c7x-mma-tidl/ti_dl/test/testvecs/input/labels.txt'
+models_base_path = '../../../..//onnx_models'
+
+def load_labels(filename):
+  with open(filename, 'r') as f:
+    return [line.strip() for line in f.readlines()]
+
+def get_class_labels(output, org_image_rgb):
+    output = np.squeeze(np.float32(output)) 
+    source_img = org_image_rgb.convert("RGBA")
+    draw = ImageDraw.Draw(source_img)
+
+    outputoffset = 0 if(output.shape[0] == 1001) else 1 
+    top_k = output.argsort()[-5:][::-1]
+    labels = load_labels(lables)
+    for j, k in enumerate(top_k):
+        curr_class = f'\n  {j}  {output[k]:08.6f}  {labels[k+outputoffset]} \n'
+        classes = classes + curr_class if ('classes' in locals()) else curr_class 
+    draw.text((0,0), classes, fill='red')
+    source_img = source_img.convert("RGB")
+    classes = classes.replace("\n", ",")
+    return(classes, source_img)
+
+colors_list = [
+( 255, 	 0,	  0 ), ( 0	 , 255,    0 ), ( 0	,   0,	 255 ), ( 255, 255,	    0  ), ( 0	 , 255,  255  ), ( 255,   0,	 255  ),
+( 255, 	 64,  0 ), ( 64	 , 255,    0 ), ( 64,   0,	 255 ), ( 255, 255,	   64  ), ( 64	 , 255,  255  ), ( 255,   64,	 255  ),
+( 196, 	128,  0 ), ( 128 , 196,    0 ), ( 128,  0,	 196 ), ( 196, 196,	  128  ), ( 128	 , 196,  196  ), ( 196,   128,	 196  ),
+( 64, 	128,  0 ), ( 128 , 64,     0 ), ( 128,  0,	 64  ), ( 196,   0,    0  ), ( 196	 ,  64,   64  ), ( 64,    196,	  64  ),
+( 64,   255, 64 ), ( 64	 , 64,   255 ),( 255, 64,	 64  ), (128,  255,   128  ), ( 128	, 128,    255  ),( 255,   128,	 128  ),
+( 196,  64, 196 ), ( 196, 196,    64 ),( 64,  196,	196  ), (196,  255,   196  ), ( 196	, 196,    255  ),( 196,   196,	 128  )]
+
+def mask_transform(inp):
+    colors = np.asarray(colors_list)
+    inp = np.squeeze(inp)
+    colorimg = np.zeros((inp.shape[0], inp.shape[1], 3), dtype=np.float32)
+    height, width = inp.shape
+    inp = np.rint(inp)
+    inp = inp.astype(np.uint8)
+    for y in range(height):
+        for x in range(width):
+            if(inp[y][x] < 22):
+                colorimg[y][x] = colors[inp[y][x]]
+    inp = colorimg.astype(np.uint8)
+    return inp
+
+def RGB2YUV( rgb ):
+    m = np.array([[ 0.29900, -0.16874,  0.50000],
+                 [0.58700, -0.33126, -0.41869],
+                 [ 0.11400, 0.50000, -0.08131]])
+    yuv = np.dot(rgb,m)
+    yuv[:,:, 1:] += 128.0
+    rgb = np.clip(yuv, 0.0, 255.0)
+    return yuv
+
+def YUV2RGB( yuv ):
+    m = np.array([[ 1.0, 1.0, 1.0],
+                 [-0.000007154783816076815, -0.3441331386566162, 2.0320025777816772],
+                 [ 1.14019975662231445, -0.5811380310058594 , 0.00001542569043522235] ])
+    yuv[:,:, 1:] -= 128.0
+    rgb = np.dot(yuv,m)
+    rgb = np.clip(rgb, 0.0, 255.0)
+    return rgb
+
+def seg_mask_overlay(output_data, org_image_rgb):
+  classes = ''
+  output_data = np.squeeze(output_data)
+  if (output_data.ndim > 2) :
+    output_data = output_data.argmax(axis=2)
+  output_data = np.squeeze(output_data)
+  mask_image_rgb  = mask_transform(output_data) 
+  org_image  = RGB2YUV(org_image_rgb)
+  mask_image = RGB2YUV(mask_image_rgb)
+  
+  org_image[:,:, 1] = mask_image[:,:, 1]
+  org_image[:,:, 2] = mask_image[:,:, 2]
+  blend_image = YUV2RGB(org_image)
+  blend_image = blend_image.astype(np.uint8)
+  blend_image = Image.fromarray(blend_image).convert('RGB')
+  
+  return(classes, blend_image)
+
+def det_box_overlay(outputs, org_image_rgb):
+    classes = ''
+    source_img = org_image_rgb.convert("RGBA")
+    draw = ImageDraw.Draw(source_img)
+    for i in range(int(outputs[3][0])):
+        if(outputs[2][0][i] > 0.1) :
+            ymin = outputs[0][0][i][0]
+            xmin = outputs[0][0][i][1]
+            ymax = outputs[0][0][i][2]
+            xmax = outputs[0][0][i][3]
+            draw.rectangle(((int(xmin*source_img.width), int(ymin*source_img.height)), (int(xmax*source_img.width), int(ymax*source_img.height))), outline = colors_list[int(outputs[1][0][i])%len(colors_list)], width=2)
+    
+    source_img = source_img.convert("RGB")
+    return(classes, source_img)
+
+
+mlperf_models_configs = {
+    'squeezenet' : {
+        'model_path' : '../../csharp/testdata/squeezenet.onnx',
+        'dataset_list' : os.path.join(dataset_base,'tflite-test-data/tidl-dataset-lite/imagenet_1000/val_1000.txt'),
+        'mean': [0, 0, 0],
+        'std' : [1, 1, 1],
+        'num_images' : numImages,
+        'num_classes': 1000,
+        'model_type': 'classification'
+    },
+    'squeezenet1.1' : {
+        'model_path' : os.path.join(models_base_path, 'squeezenet1.1.onnx'),
+        'dataset_list' : os.path.join(dataset_base,'tflite-test-data/tidl-dataset-lite/imagenet_1000/val_1000.txt'),
+        'mean': [123.675, 116.28, 103.53],
+        'std' : [0.017125, 0.017507, 0.017429],
+        'num_images' : numImages,
+        'num_classes': 1000,
+        'model_type': 'classification'
+    },
+    'mobilenetv2-1.0' : {
+        'model_path' : os.path.join(models_base_path, 'mobilenetv2-1.0.onnx'),
+        'dataset_list' : os.path.join(dataset_base,'tflite-test-data/tidl-dataset-lite/imagenet_1000/val_1000.txt'),
+        'mean': [123.675, 116.28, 103.53],
+        'std' : [0.017125, 0.017507, 0.017429],
+        'num_images' : numImages,
+        'num_classes': 1000,
+        'model_type': 'classification'
+   },
+    'deeplabv3_mnv2_ade20k_float' : {
+        'model_path' : os.path.join(models_base_path,'deeplabv3_mnv2_ade20k_float.tflite'),
+        'dataset_list' : os.path.join(dataset_base,'tflite-test-data/tidl-dataset-lite/ADEChallengeData2016Val/seg_val_list.txt'),
+        'mean': [127.5, 127.5, 127.5],
+        'std' : [1/127.5, 1/127.5, 1/127.5],
+        'num_images' : numImages,
+        'num_classes': 32,
+        'model_type': 'seg'
+    },
+    'ssd_mobilenet_v1_coco_2018_01_28' : {
+        'model_path' : os.path.join(models_base_path,'ssd_mobilenet_v1_coco_2018_01_28_th_0p3.tflite'),
+        'mean': [127.5, 127.5, 127.5],
+        'std' : [1/127.5, 1/127.5, 1/127.5],
+        'num_images' : numImages,
+        'num_classes': 91,
+        'model_type': 'od'
+    },
+    'ssd_mobilenet_v2_coco_2018_03_29' : {
+        'model_path' : os.path.join(models_base_path,'ssd_mobilenet_v2_coco_2018_03_29.tflite'),
+        'mean': [127.5, 127.5, 127.5],
+        'std' : [1/127.5, 1/127.5, 1/127.5],
+        'num_images' : numImages,
+        'num_classes': 91,
+        'model_type': 'od'
+    },
+    'ssd_mobilenet_v2_300_float' : {
+        'model_path' : os.path.join(models_base_path,'ssd_mobilenet_v2_300_float.tflite'),
+        'mean': [127.5, 127.5, 127.5],
+        'std' : [1/127.5, 1/127.5, 1/127.5],
+        'num_images' : numImages,
+        'num_classes': 91,
+        'model_type': 'od'
+    },
+}
diff --git a/tidl_demos/classification_demo/onnxrt_tidl.py b/tidl_demos/classification_demo/onnxrt_tidl.py
new file mode 100644
index 000000000..fb5e3c7d1
--- /dev/null
+++ b/tidl_demos/classification_demo/onnxrt_tidl.py
@@ -0,0 +1,160 @@
+import numpy as np
+import onnxruntime as rt
+from PIL import Image
+import time
+import argparse
+import os
+import platform
+from models import mlperf_models_configs, get_class_labels, seg_mask_overlay, det_box_overlay, required_options, optional_options, models_base_path
+
+parser = argparse.ArgumentParser()
+parser.add_argument('-c','--compile', action='store_true', help='Run in Model compilation mode')
+parser.add_argument('-d','--disable_offload', action='store_true',  help='Disable offload to TIDL')
+args = parser.parse_args()
+#os.environ["TIDL_RT_PERFSTATS"] = "1"
+
+TIDL_BASE_PATH = '/home/a0393754/work/onnxrt_7.3_rel/c7x-mma-tidl'
+image_path = '../img/mushroom.png'
+#image_path = '../../../tidl/c7x-mma-tidl/ti_dl/test/testvecs/input/airshow.jpg'
+model_path = '../../csharp/testdata/squeezenet.onnx'
+lables = '/home/a0393754/work/onnxrt_7.3_rel/c7x-mma-tidl/ti_dl/test/testvecs/input/labels.txt'
+
+
+so = rt.SessionOptions()
+
+print("Available execution providers : ", rt.get_available_providers())
+print("Platform - ", platform.python_implementation())
+#so.log_severity_level = 0
+#so.log_verbosity_level = 4
+#so.enable_profiling = True
+
+delegate_options = {}
+delegate_options.update(required_options)
+delegate_options.update(optional_options)
+
+
+calib_images = [os.path.join(TIDL_BASE_PATH,'ti_dl/test/testvecs/input/airshow.jpg'),
+                os.path.join(TIDL_BASE_PATH,'ti_dl/test/testvecs/input/ADE_val_00001801.jpg')]
+class_test_images = [os.path.join(TIDL_BASE_PATH,'ti_dl/test/testvecs/input/airshow.jpg')]
+od_test_images    = [os.path.join(TIDL_BASE_PATH,'ti_dl/test/testvecs/input/ADE_val_00001801.jpg')]
+seg_test_images   = [os.path.join(TIDL_BASE_PATH,'ti_dl/test/testvecs/input/ADE_val_00001801.jpg')]
+
+def infer_image(sess, image_file, config):
+  input_details = sess.get_inputs()
+  input_name = input_details[0].name
+  floating_model = (input_details[0].type == 'tensor(float)')
+  height = input_details[0].shape[2]
+  width  = input_details[0].shape[3]
+  img    = Image.open(image_file).convert('RGB').resize((width, height))
+  input_data = np.expand_dims(img, axis=0)
+  input_data = np.transpose(input_data, (0, 3, 1, 2))
+
+  if floating_model:
+    input_data = np.float32(input_data)
+    for mean, scale, ch in zip(config['mean'], config['std'], range(input_data.shape[1])):
+        input_data[:,ch,:,:] = ((input_data[:,ch,:,:]- mean) * scale)
+  
+  start_time = time.time()
+  #interpreter invoke call
+  output = sess.run(None, {input_name: input_data})[0]
+  #prof_file = sess.end_profiling()
+  #print(prof_file)
+  stop_time = time.time()
+  infer_time = stop_time - start_time
+
+  benchmark_dict = sess.get_TI_benchmark_data()
+  print(benchmark_dict['ts:run_end'] - benchmark_dict['ts:run_start'])
+  print(benchmark_dict)
+
+  #outputs = [interpreter.get_tensor(output_detail['index']) for output_detail in output_details]
+  return img, output, infer_time, 0
+
+def run_model(model, mIdx, log_file):
+    print("\n Running model  ", model)
+    config = mlperf_models_configs[model]
+    #set input images for demo
+    config = mlperf_models_configs[model]
+    if config['model_type'] == 'classification':
+        test_images = class_test_images
+    elif config['model_type'] == 'od':
+        test_images = od_test_images
+    elif config['model_type'] == 'seg':
+        test_images = seg_test_images
+    
+    delegate_options['artifacts_folder'] = delegate_options['artifacts_folder'] + '/' + model + '/'
+    
+    # delete the contents of this folder
+    if args.compile:
+        os.makedirs(delegate_options['artifacts_folder'], exist_ok=True)
+        for root, dirs, files in os.walk(delegate_options['artifacts_folder'], topdown=False):
+            [os.remove(os.path.join(root, f)) for f in files]
+            [os.rmdir(os.path.join(root, d)) for d in dirs]
+
+    if(args.compile == True):
+        delegate_options['import'] = 'yes'
+        input_image = calib_images
+    else:
+        input_image = test_images
+    
+    numFrames = config['num_images']
+    if(delegate_options['import'] == 'yes'):
+        if numFrames > delegate_options['tidl_calibration_options:num_frames_calibration']:
+            numFrames = delegate_options['tidl_calibration_options:num_frames_calibration']
+    
+    ############   set interpreter  ################################
+    if args.disable_offload : 
+        EP_list = ['CPUExecutionProvider']
+        sess = rt.InferenceSession(config['model_path'] , providers=EP_list,sess_options=so)
+    elif args.compile:
+        EP_list = ['TIDLCompilationProvider','CPUExecutionProvider']
+        sess = rt.InferenceSession(config['model_path'] ,providers=EP_list, provider_options=[delegate_options, {}], sess_options=so)
+    else:
+        EP_list = ['TIDLExecutionProvider','CPUExecutionProvider']
+        sess = rt.InferenceSession(config['model_path'] ,providers=EP_list, provider_options=[delegate_options, {}], sess_options=so)
+    ################################################################
+    
+    # run session
+    for i in range(numFrames):
+        #img, output, proc_time, sub_graph_time = infer_image(sess, input_image[i%len(input_image)], config)
+        img, output, proc_time, sub_graph_time = infer_image(sess, input_image[i%len(input_image)], config)
+        total_proc_time = total_proc_time + proc_time if ('total_proc_time' in locals()) else proc_time
+        sub_graphs_time = sub_graphs_time + sub_graph_time if ('sub_graphs_time' in locals()) else sub_graph_time
+    
+    #total_proc_time = total_proc_time/1000000
+    total_proc_time = total_proc_time * 1000
+    sub_graphs_time = sub_graphs_time/1000000
+
+    # output post processing
+    if(args.compile == False):  # post processing enabled only for inference
+        if config['model_type'] == 'classification':
+            classes, image = get_class_labels(output[0],img)
+            print(classes)
+        elif config['model_type'] == 'od':
+            classes, image = det_box_overlay(output, img)
+        elif config['model_type'] == 'seg':
+            classes, image = seg_mask_overlay(output[0], img)
+        else:
+            print("Not a valid model type")
+
+        #print("Saving image to ", delegate_options['artifacts_folder'])
+        #image.save(delegate_options['artifacts_folder'] + "post_proc_out_"+os.path.basename(config['model_path'])+'_'+os.path.basename(input_image[i%len(input_image)]), "JPEG") 
+    
+    log = f'\n \n   #{mIdx+1:5d}, {model:50s}, Total time : {total_proc_time/(i+1):10.1f}, Offload Time : {sub_graphs_time/(i+1):10.1f} \n \n ' #{classes} \n \n'
+    print(log) 
+    log_file.write(log)
+
+
+log_file = open("log.txt", "w+", buffering=1)
+
+#models = mlperf_models_configs.keys()
+#models=['mobilenetv2-1.0']
+models = ['squeezenet']
+
+log = f'Running {len(models)} Models - {models}\n'
+print(log)
+log_file.write(log)
+
+for mIdx, model in enumerate(models):
+    run_model(model, mIdx, log_file)
+
+log_file.close()
diff --git a/tidl_demos/classification_demo/setenv.sh b/tidl_demos/classification_demo/setenv.sh
new file mode 100755
index 000000000..e1883e844
--- /dev/null
+++ b/tidl_demos/classification_demo/setenv.sh
@@ -0,0 +1,17 @@
+#!/bin/bash
+
+################################################################################
+# Ensure script is only run sourced
+if [[ $_ == $0 ]]; then
+    echo "!!!ERROR!!! - PLEASE RUN WITH 'source'"
+    exit 1
+fi
+
+################################################################################
+# Set environment variables
+#ORT_LIB_PATH=../../build/Linux/RelWithDebInfo/
+ORT_LIB_PATH=../../build/Linux/Debug/
+
+LD_LIBRARY_PATH_LOCAL=$ORT_LIB_PATH
+export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$LD_LIBRARY_PATH_LOCAL
+export TIDL_BASE_PATH=../../../tidl_it_onnxRt/c7x-mma-tidl
diff --git a/tidl_demos/classification_demo/validator.cc b/tidl_demos/classification_demo/validator.cc
new file mode 100644
index 000000000..b31318122
--- /dev/null
+++ b/tidl_demos/classification_demo/validator.cc
@@ -0,0 +1,149 @@
+#include <assert.h>
+#include <getopt.h>
+#include <iostream>
+#include <cstdarg>
+#include <cstdio>
+#include <fstream>
+#include <numeric>
+#include <algorithm>
+#include <functional>
+#include <vector>
+#include <limits>
+#include <stdexcept>
+#include <libgen.h>
+#include <utility>
+#include <sys/time.h>
+
+#include <onnxruntime/core/session/onnxruntime_cxx_api.h>
+#include <onnxruntime/core/providers/tidl/tidl_provider_factory.h>
+#include <onnxruntime/core/providers/dnnl/dnnl_provider_factory.h>
+
+#include "validator.h"
+
+Validator::Validator(Ort::Env& env,
+                     std::string model_path,
+                     std::string labels_path,
+                     Ort::SessionOptions& session_options,
+                     std::vector<uint8_t>& image_data)
+    : _session(env, model_path.c_str(), session_options),
+      _num_input_nodes{_session.GetInputCount()},
+      _input_node_names(_num_input_nodes),
+      _image_data(image_data),
+      _labels_path(labels_path)
+{
+    Validate();
+}
+
+int Validator::GetImageSize() const
+{
+    return _image_size;
+}
+
+void Validator::PrepareInputs()
+{
+    Ort::AllocatorWithDefaultOptions allocator;
+    
+    printf("Number of inputs = %zu\n", _num_input_nodes);
+    
+    // iterate over all input nodes
+    for (int i = 0; i < _num_input_nodes; i++) {
+        // print input node names
+        char* input_name = _session.GetInputName(i, allocator);
+        printf("Input %d : name=%s\n", i, input_name);
+        _input_node_names[i] = input_name;
+        
+        // print input node types
+        Ort::TypeInfo type_info = _session.GetInputTypeInfo(i);
+        auto tensor_info = type_info.GetTensorTypeAndShapeInfo();
+        
+        ONNXTensorElementDataType type = tensor_info.GetElementType();
+        printf("Input %d : type=%d\n", i, type);
+        
+        // print input shapes/dims
+        _input_node_dims = tensor_info.GetShape();
+        printf("Input %d : num_dims=%zu\n", i, _input_node_dims.size());
+        for (int j = 0; j < _input_node_dims.size(); j++)
+        {
+            printf("Input %d : dim %d=%jd\n", i, j, _input_node_dims[j]);
+        }
+    }
+}
+
+void Validator::ScoreModel()
+{
+    //*************************************************************************
+    // Score the model using sample data, and inspect values
+
+    size_t input_tensor_size = 224 * 224 * 3;  // simplify ... using known dim values to calculate size
+    // use OrtGetTensorShapeElementCount() to get official size!
+
+    // std::vector<float> input_tensor_values(input_tensor_size);
+    std::vector<const char*> output_node_names = {"softmaxout_1"};
+
+    // create input tensor object from data values
+    auto memory_info = Ort::MemoryInfo::CreateCpu(OrtArenaAllocator, OrtMemTypeDefault);
+
+    std::vector<float> floatvec(_image_data.begin(), _image_data.end());
+    Ort::Value input_tensor = Ort::Value::CreateTensor<float>(memory_info, floatvec.data(), input_tensor_size, _input_node_dims.data(), 4);
+    assert(input_tensor.IsTensor());
+
+    // score model & input tensor, get back output tensor
+    auto run_options = Ort::RunOptions();
+    run_options.SetRunLogVerbosityLevel(2);
+    
+    auto output_tensors = _session.Run(run_options, _input_node_names.data(), &input_tensor, 1, output_node_names.data(), 1);
+    assert(output_tensors.size() == 1 && output_tensors.front().IsTensor());
+
+    // Get pointer to output tensor float values
+    float* floatarr = output_tensors.front().GetTensorMutableData<float>();
+
+    // Determine most common index
+    float max_val = 0.0;
+    int max_index = 0;
+    for (int i = 0; i < 1000; i++)
+    {
+        if (floatarr[i] > max_val)
+        {
+            max_val = floatarr[i];
+            max_index = i;
+        }
+    }
+    std::cout << "MAX: class [" << max_index << "] = " << max_val << std::endl;
+
+    std::vector<std::string> labels = ReadFileToVec(_labels_path);
+    std::cout << labels[max_index] << std::endl;
+}
+
+void Validator::Validate()
+{
+    PrepareInputs();
+    ScoreModel();
+}
+
+std::vector<std::string> Validator::ReadFileToVec(std::string fname)
+{
+    // Open the File
+    std::ifstream file(fname.c_str());
+  
+    // Check if object is valid
+    if(!file)
+    {
+        throw std::runtime_error("Cannot open file: " + fname);
+    }
+
+    // Read the next line from File untill it reaches the end.
+    std::string line;
+    std::vector<std::string> labels;
+    while (std::getline(file, line))
+    {
+        // Line contains string of length > 0 then save it in vector
+        if(!line.empty())
+        {
+            labels.push_back(line);
+        }
+    }
+  
+    // Close The File
+    file.close();
+    return labels;
+}
diff --git a/tidl_demos/classification_demo/validator.h b/tidl_demos/classification_demo/validator.h
new file mode 100644
index 000000000..4170fef12
--- /dev/null
+++ b/tidl_demos/classification_demo/validator.h
@@ -0,0 +1,32 @@
+#ifndef _VALIDATOR_H
+#define _VALIDATOR_H
+
+#include <onnxruntime/core/session/onnxruntime_cxx_api.h>
+
+class Validator {
+private:
+    // ORT Session
+    Ort::Session _session;
+
+    // Input information
+    size_t _num_input_nodes;
+    std::vector<const char*> _input_node_names;
+    std::vector<int64_t> _input_node_dims;
+    std::vector<uint8_t> _image_data;
+
+    int _image_size;
+    std::string _labels_path;
+
+    void PrepareInputs();
+    void ScoreModel();
+    void Validate();
+    
+    std::vector<std::string> ReadFileToVec(std::string fname);
+
+public:
+    int GetImageSize() const;
+    Validator(Ort::Env& env, std::string model_path, std::string labels_path,
+              Ort::SessionOptions& session_options, std::vector<uint8_t>& image_data);
+};
+
+#endif // _VALIDATOR_H
diff --git a/tidl_demos/labels/imgnet_labels.txt b/tidl_demos/labels/imgnet_labels.txt
new file mode 100644
index 000000000..8fc610df0
--- /dev/null
+++ b/tidl_demos/labels/imgnet_labels.txt
@@ -0,0 +1,1000 @@
+Tench Tinca Tinca
+Goldfish Carassius Auratus
+Great White Shark White Shark Man-Eater Man-Eating Shark Carcharodon Carcharias
+Tiger Shark Galeocerdo Cuvieri
+Hammerhead Hammerhead Shark
+Electric Ray Crampfish Numbfish Torpedo
+Stingray
+Cock
+Hen
+Ostrich Struthio Camelus
+Brambling Fringilla Montifringilla
+Goldfinch Carduelis Carduelis
+House Finch Linnet Carpodacus Mexicanus
+Junco Snowbird
+Indigo Bunting Indigo Finch Indigo Bird Passerina Cyanea
+Robin American Robin Turdus Migratorius
+Bulbul
+Jay
+Magpie
+Chickadee
+Water Ouzel Dipper
+Kite
+Bald Eagle American Eagle Haliaeetus Leucocephalus
+Vulture
+Great Grey Owl Great Gray Owl Strix Nebulosa
+European Fire Salamander Salamandra Salamandra
+Common Newt Triturus Vulgaris
+Eft
+Spotted Salamander Ambystoma Maculatum
+Axolotl Mud Puppy Ambystoma Mexicanum
+Bullfrog Rana Catesbeiana
+Tree Frog Tree-Frog
+Tailed Frog Bell Toad Ribbed Toad Tailed Toad Ascaphus Trui
+Loggerhead Loggerhead Turtle Caretta Caretta
+Leatherback Turtle Leatherback Leathery Turtle Dermochelys Coriacea
+Mud Turtle
+Terrapin
+Box Turtle Box Tortoise
+Banded Gecko
+Common Iguana Iguana Iguana Iguana
+American Chameleon Anole Anolis Carolinensis
+Whiptail Whiptail Lizard
+Agama
+Frilled Lizard Chlamydosaurus Kingi
+Alligator Lizard
+Gila Monster Heloderma Suspectum
+Green Lizard Lacerta Viridis
+African Chameleon Chamaeleo Chamaeleon
+Komodo Dragon Komodo Lizard Dragon Lizard Giant Lizard Varanus Komodoensis
+African Crocodile Nile Crocodile Crocodylus Niloticus
+American Alligator Alligator Mississipiensis
+Triceratops
+Thunder Snake Worm Snake Carphophis Amoenus
+Ringneck Snake Ring-Necked Snake Ring Snake
+Hognose Snake Puff Adder Sand Viper
+Green Snake Grass Snake
+King Snake Kingsnake
+Garter Snake Grass Snake
+Water Snake
+Vine Snake
+Night Snake Hypsiglena Torquata
+Boa Constrictor Constrictor Constrictor
+Rock Python Rock Snake Python Sebae
+Indian Cobra Naja Naja
+Green Mamba
+Sea Snake
+Horned Viper Cerastes Sand Viper Horned Asp Cerastes Cornutus
+Diamondback Diamondback Rattlesnake Crotalus Adamanteus
+Sidewinder Horned Rattlesnake, Crotalus Cerastes
+Trilobite
+Harvestman Daddy Longlegs Phalangium Opilio
+Scorpion
+Black And Gold Garden Spider Argiope Aurantia
+Barn Spider Araneus Cavaticus
+Garden Spider Aranea Diademata
+Black Widow Latrodectus Mactans
+Tarantula
+Wolf Spider Hunting Spider
+Tick
+Centipede
+Black Grouse
+Ptarmigan
+Ruffed Grouse Partridge Bonasa Umbellus
+Prairie Chicken Prairie Grouse Prairie Fowl
+Peacock
+Quail
+Partridge
+African Grey African Gray Psittacus Erithacus
+Macaw
+Sulphur-Crested Cockatoo Kakatoe Galerita Cacatua Galerita
+Lorikeet
+Coucal
+Bee Eater
+Hornbill
+Hummingbird
+Jacamar
+Toucan
+Drake
+Red-Breasted Merganser Mergus Serrator
+Goose
+Black Swan Cygnus Atratus
+Tusker
+Echidna Spiny Anteater Anteater
+Platypus Duckbill Duckbilled Platypus Duck-Billed Platypus Ornithorhynchus Anatinus
+Wallaby Brush Kangaroo
+Koala Koala Bear Kangaroo Bear Native Bear Phascolarctos Cinereus
+Wombat
+Jellyfish
+Sea Anemone Anemone
+Brain Coral
+Flatworm Platyhelminth
+Nematode Nematode Worm Roundworm
+Conch
+Snail
+Slug
+Sea Slug Nudibranch
+Chiton Coat-Of-Mail Shell Sea Cradle Polyplacophore
+Chambered Nautilus Pearly Nautilus Nautilus
+Dungeness Crab Cancer Magister
+Rock Crab Cancer Irroratus
+Fiddler Crab
+King Crab Alaska Crab Alaskan King Crab Alaska King Crab Paralithodes Camtschatica
+American Lobster Northern Lobster Maine Lobster Homarus Americanus
+Spiny Lobster Langouste Rock Lobster Crawfish Crayfish Sea Crawfish
+Crayfish Crawfish Crawdad Crawdaddy
+Hermit Crab
+Isopod
+White Stork Ciconia Ciconia
+Black Stork Ciconia Nigra
+Spoonbill
+Flamingo
+Little Blue Heron Egretta Caerulea
+American Egret Great White Heron Egretta Albus
+Bittern
+Crane
+Limpkin Aramus Pictus
+European Gallinule Porphyrio Porphyrio
+American Coot Marsh Hen Mud Hen Water Hen Fulica Americana
+Bustard
+Ruddy Turnstone Arenaria Interpres
+Red-Backed Sandpiper Dunlin Erolia Alpina
+Redshank Tringa Totanus
+Dowitcher
+Oystercatcher Oyster Catcher
+Pelican
+King Penguin Aptenodytes Patagonica
+Albatross Mollymawk
+Grey Whale Gray Whale Devilfish Eschrichtius Gibbosus Eschrichtius Robustus
+Killer Whale Killer Orca Grampus Sea Wolf Orcinus Orca
+Dugong Dugong Dugon
+Sea Lion
+Chihuahua
+Japanese Spaniel
+Maltese Dog Maltese Terrier Maltese
+Pekinese Pekingese Peke
+Shih-Tzu
+Blenheim Spaniel
+Papillon
+Toy Terrier
+Rhodesian Ridgeback
+Afghan Hound Afghan
+Basset Basset Hound
+Beagle
+Bloodhound Sleuthhound
+Bluetick
+Black-And-Tan Coonhound
+Walker Hound Walker Foxhound
+English Foxhound
+Redbone
+Borzoi Russian Wolfhound
+Irish Wolfhound
+Italian Greyhound
+Whippet
+Ibizan Hound Ibizan Podenco
+Norwegian Elkhound Elkhound
+Otterhound Otter Hound
+Saluki Gazelle Hound
+Scottish Deerhound Deerhound
+Weimaraner
+Staffordshire Bullterrier Staffordshire Bull Terrier
+American Staffordshire Terrier Staffordshire Terrier American Pit Bull Terrier Pit Bull Terrier
+Bedlington Terrier
+Border Terrier
+Kerry Blue Terrier
+Irish Terrier
+Norfolk Terrier
+Norwich Terrier
+Yorkshire Terrier
+Wire-Haired Fox Terrier
+Lakeland Terrier
+Sealyham Terrier Sealyham
+Airedale Airedale Terrier
+Cairn Cairn Terrier
+Australian Terrier
+Dandie Dinmont Dandie Dinmont Terrier
+Boston Bull Boston Terrier
+Miniature Schnauzer
+Giant Schnauzer
+Standard Schnauzer
+Scotch Terrier Scottish Terrier Scottie
+Tibetan Terrier Chrysanthemum Dog
+Silky Terrier Sydney Silky
+Soft-Coated Wheaten Terrier
+West Highland White Terrier
+Lhasa Lhasa Apso
+Flat-Coated Retriever
+Curly-Coated Retriever
+Golden Retriever
+Labrador Retriever
+Chesapeake Bay Retriever
+German Short-Haired Pointer
+Vizsla Hungarian Pointer
+English Setter
+Irish Setter Red Setter
+Gordon Setter
+Brittany Spaniel
+Clumber Clumber Spaniel
+English Springer English Springer Spaniel
+Welsh Springer Spaniel
+Cocker Spaniel English Cocker Spaniel Cocker
+Sussex Spaniel
+Irish Water Spaniel
+Kuvasz
+Schipperke
+Groenendael
+Malinois
+Briard
+Kelpie
+Komondor
+Old English Sheepdog Bobtail
+Shetland Sheepdog Shetland Sheep Dog Shetland
+Collie
+Border Collie
+Bouvier Des Flandres Bouviers Des Flandres
+Rottweiler
+German Shepherd German Shepherd Dog German Police Dog Alsatian
+Doberman Doberman Pinscher
+Miniature Pinscher
+Greater Swiss Mountain Dog
+Bernese Mountain Dog
+Appenzeller
+Entlebucher
+Boxer
+Bull Mastiff
+Tibetan Mastiff
+French Bulldog
+Great Dane
+Saint Bernard St Bernard
+Eskimo Dog Husky
+Malamute Malemute Alaskan Malamute
+Siberian Husky
+Dalmatian Coach Dog Carriage Dog
+Affenpinscher Monkey Pinscher Monkey Dog
+Basenji
+Pug Pug-Dog
+Leonberg
+Newfoundland Newfoundland Dog
+Great Pyrenees
+Samoyed Samoyede
+Pomeranian
+Chow Chow Chow
+Keeshond
+Brabancon Griffon
+Pembroke Pembroke Welsh Corgi
+Cardigan Cardigan Welsh Corgi
+Toy Poodle
+Miniature Poodle
+Standard Poodle
+Mexican Hairless
+Timber Wolf Grey Wolf Gray Wolf Canis Lupus
+White Wolf Arctic Wolf Canis Lupus Tundrarum
+Red Wolf Maned Wolf Canis Rufus Canis Niger
+Coyote Prairie Wolf Brush Wolf Canis Latrans
+Dingo Warrigal Warragal Canis Dingo
+Dhole Cuon Alpinus
+African Hunting Dog Hyena Dog Cape Hunting Dog Lycaon Pictus
+Hyena Hyaena
+Red Fox Vulpes Vulpes
+Kit Fox Vulpes Macrotis
+Arctic Fox White Fox Alopex Lagopus
+Grey Fox Gray Fox Urocyon Cinereoargenteus
+Tabby Tabby Cat
+Tiger Cat
+Persian Cat
+Siamese Cat Siamese
+Egyptian Cat
+Cougar Puma Catamount Mountain Lion Painter Panther Felis Concolor
+Lynx Catamount
+Leopard Panthera Pardus
+Snow Leopard Ounce Panthera Uncia
+Jaguar Panther Panthera Onca Felis Onca
+Lion King Of Beasts Panthera Leo
+Tiger Panthera Tigris
+Cheetah Chetah Acinonyx Jubatus
+Brown Bear Bruin Ursus Arctos
+American Black Bear Black Bear Ursus Americanus Euarctos Americanus
+Ice Bear Polar Bear Ursus Maritimus Thalarctos Maritimus
+Sloth Bear Melursus Ursinus Ursus Ursinus
+Mongoose
+Meerkat Mierkat
+Tiger Beetle
+Ladybug Ladybeetle Lady Beetle Ladybird Ladybird Beetle
+Ground Beetle Carabid Beetle
+Long-Horned Beetle Longicorn Longicorn Beetle
+Leaf Beetle Chrysomelid
+Dung Beetle
+Rhinoceros Beetle
+Weevil
+Fly
+Bee
+Ant Emmet Pismire
+Grasshopper Hopper
+Cricket
+Walking Stick Walkingstick Stick Insect
+Cockroach Roach
+Mantis Mantid
+Cicada Cicala
+Leafhopper
+Lacewing Lacewing Fly
+Dragonfly Darning Needle Devils Darning Needle Sewing Needle Snake Feeder Snake Doctor Mosquito Hawk Skeeter Hawk
+Damselfly
+Admiral
+Ringlet Ringlet Butterfly
+Monarch Monarch Butterfly Milkweed Butterfly Danaus Plexippus
+Cabbage Butterfly
+Sulphur Butterfly Sulfur Butterfly
+Lycaenid Lycaenid Butterfly
+Starfish Sea Star
+Sea Urchin
+Sea Cucumber Holothurian
+Wood Rabbit Cottontail Cottontail Rabbit
+Hare
+Angora Angora Rabbit
+Hamster
+Porcupine Hedgehog
+Fox Squirrel Eastern Fox Squirrel Sciurus Niger
+Marmot
+Beaver
+Guinea Pig Cavia Cobaya
+Sorrel
+Zebra
+Hog Pig Grunter Squealer Sus Scrofa
+Wild Boar Boar Sus Scrofa
+Warthog
+Hippopotamus Hippo River Horse Hippopotamus Amphibius
+Ox
+Water Buffalo Water Ox Asiatic Buffalo Bubalus Bubalis
+Bison
+Ram Tup
+Bighorn Bighorn Sheep Cimarron Rocky Mountain Bighorn Rocky Mountain Sheep Ovis Canadensis
+Ibex Capra Ibex
+Hartebeest
+Impala Aepyceros Melampus
+Gazelle
+Arabian Camel Dromedary Camelus Dromedarius
+Llama
+Weasel
+Mink
+Polecat Fitch Foulmart Foumart Mustela Putorius
+Black-Footed Ferret Ferret Mustela Nigripes
+Otter
+Skunk Polecat Wood Pussy
+Badger
+Armadillo
+Three-Toed Sloth Ai Bradypus Tridactylus
+Orangutan Orang Orangutang Pongo Pygmaeus
+Gorilla Gorilla Gorilla
+Chimpanzee Chimp Pan Troglodytes
+Gibbon Hylobates Lar
+Siamang Hylobates Syndactylus Symphalangus Syndactylus
+Guenon Guenon Monkey
+Patas Hussar Monkey Erythrocebus Patas
+Baboon
+Macaque
+Langur
+Colobus Colobus Monkey
+Proboscis Monkey Nasalis Larvatus
+Marmoset
+Capuchin Ringtail Cebus Capucinus
+Howler Monkey Howler
+Titi Titi Monkey
+Spider Monkey Ateles Geoffroyi
+Squirrel Monkey Saimiri Sciureus
+Madagascar Cat Ring-Tailed Lemur Lemur Catta
+Indri Indris Indri Indri Indri Brevicaudatus
+Indian Elephant Elephas Maximus
+African Elephant Loxodonta Africana
+Lesser Panda Red Panda Panda Bear Cat Cat Bear Ailurus Fulgens
+Giant Panda Panda Panda Bear Coon Bear Ailuropoda Melanoleuca
+Barracouta Snoek
+Eel
+Coho Cohoe Coho Salmon Blue Jack Silver Salmon Oncorhynchus Kisutch
+Rock Beauty Holocanthus Tricolor
+Anemone Fish
+Sturgeon
+Gar Garfish Garpike Billfish Lepisosteus Osseus
+Lionfish
+Puffer Pufferfish Blowfish Globefish
+Abacus
+Abaya
+Academic Gown Academic Robe Judges Robe
+Accordion Piano Accordion Squeeze Box
+Acoustic Guitar
+Aircraft Carrier Carrier Flattop Attack Aircraft Carrier
+Airliner
+Airship Dirigible
+Altar
+Ambulance
+Amphibian Amphibious Vehicle
+Analog Clock
+Apiary Bee House
+Apron
+Ashcan Trash Can Garbage Can Wastebin Ash Bin Ash-Bin Ashbin Dustbin Trash Barrel Trash Bin
+Assault Rifle Assault Gun
+Backpack Back Pack Knapsack Packsack Rucksack Haversack
+Bakery Bakeshop Bakehouse
+Balance Beam Beam
+Balloon
+Ballpoint Ballpoint Pen Ballpen Biro
+Band Aid
+Banjo
+Bannister Banister Balustrade Balusters Handrail
+Barbell
+Barber Chair
+Barbershop
+Barn
+Barometer
+Barrel Cask
+Barrow Garden Cart Lawn Cart Wheelbarrow
+Baseball
+Basketball
+Bassinet
+Bassoon
+Bathing Cap Swimming Cap
+Bath Towel
+Bathtub Bathing Tub Bath Tub
+Beach Wagon Station Wagon Wagon Estate Car Beach Waggon Station Waggon Waggon
+Beacon Lighthouse Beacon Light Pharos
+Beaker
+Bearskin Busby Shako
+Beer Bottle
+Beer Glass
+Bell Cote Bell Cot
+Bib
+Bicycle-Built-For-Two Tandem Bicycle Tandem
+Bikini Two-Piece
+Binder Ring-Binder
+Binoculars Field Glasses Opera Glasses
+Birdhouse
+Boathouse
+Bobsled Bobsleigh Bob
+Bolo Tie Bolo Bola Tie Bola
+Bonnet Poke Bonnet
+Bookcase
+Bookshop Bookstore Bookstall
+Bottlecap
+Bow
+Bow Tie Bow-Tie Bowtie
+Brass Memorial Tablet Plaque
+Brassiere Bra Bandeau
+Breakwater Groin Groyne Mole Bulwark Seawall Jetty
+Breastplate Aegis Egis
+Broom
+Bucket Pail
+Buckle
+Bulletproof Vest
+Bullet Train Bullet
+Butcher Shop Meat Market
+Cab Hack Taxi Taxicab
+Caldron Cauldron
+Candle Taper Wax Light
+Cannon
+Canoe
+Can Opener Tin Opener
+Cardigan
+Car Mirror
+Carousel Carrousel Merry-Go-Round Roundabout Whirligig
+Carpenters Kit Tool Kit
+Carton
+Car Wheel
+Cash Machine Cash Dispenser Automated Teller Machine Automatic Teller Machine Automated Teller Automatic Teller Atm
+Cassette
+Cassette Player
+Castle
+Catamaran
+Cd Player
+Cello Violoncello
+Cellular Telephone Cellular Phone Cellphone Cell Mobile Phone
+Chain
+Chainlink Fence
+Chain Mail Ring Mail Mail Chain Armor Chain Armour Ring Armor Ring Armour
+Chain Saw Chainsaw
+Chest
+Chiffonier Commode
+Chime Bell Gong
+China Cabinet China Closet
+Christmas Stocking
+Church Church Building
+Cinema Movie Theater Movie Theatre Movie House Picture Palace
+Cleaver Meat Cleaver Chopper
+Cliff Dwelling
+Cloak
+Clog Geta Patten Sabot
+Cocktail Shaker
+Coffee Mug
+Coffeepot
+Coil Spiral Volute Whorl Helix
+Combination Lock
+Computer Keyboard Keypad
+Confectionery Confectionary Candy Store
+Container Ship Containership Container Vessel
+Convertible
+Corkscrew Bottle Screw
+Cornet Horn Trumpet Trump
+Cowboy Boot
+Cowboy Hat Ten-Gallon Hat
+Cradle
+Crane
+Crash Helmet
+Crate
+Crib Cot
+Crock Pot
+Croquet Ball
+Crutch
+Cuirass
+Dam Dike Dyke
+Desk
+Desktop Computer
+Dial Telephone Dial Phone
+Diaper Nappy Napkin
+Digital Clock
+Digital Watch
+Dining Table Board
+Dishrag Dishcloth
+Dishwasher Dish Washer Dishwashing Machine
+Disk Brake Disc Brake
+Dock Dockage Docking Facility
+Dogsled Dog Sled Dog Sleigh
+Dome
+Doormat Welcome Mat
+Drilling Platform Offshore Rig
+Drum Membranophone Tympan
+Drumstick
+Dumbbell
+Dutch Oven
+Electric Fan Blower
+Electric Guitar
+Electric Locomotive
+Entertainment Center
+Envelope
+Espresso Maker
+Face Powder
+Feather Boa Boa
+File File Cabinet Filing Cabinet
+Fireboat
+Fire Engine Fire Truck
+Fire Screen Fireguard
+Flagpole Flagstaff
+Flute Transverse Flute
+Folding Chair
+Football Helmet
+Forklift
+Fountain
+Fountain Pen
+Four-Poster
+Freight Car
+French Horn Horn
+Frying Pan Frypan Skillet
+Fur Coat
+Garbage Truck Dustcart
+Gasmask Respirator Gas Helmet
+Gas Pump Gasoline Pump Petrol Pump Island Dispenser
+Goblet
+Go-Kart
+Golf Ball
+Golfcart Golf Cart
+Gondola
+Gong Tam-Tam
+Gown
+Grand Piano Grand
+Greenhouse Nursery Glasshouse
+Grille Radiator Grille
+Grocery Store Grocery Food Market Market
+Guillotine
+Hair Slide
+Hair Spray
+Half Track
+Hammer
+Hamper
+Hand Blower Blow Dryer Blow Drier Hair Dryer Hair Drier
+Hand-Held Computer Hand-Held Microcomputer
+Handkerchief Hankie Hanky Hankey
+Hard Disc Hard Disk Fixed Disk
+Harmonica Mouth Organ Harp Mouth Harp
+Harp
+Harvester Reaper
+Hatchet
+Holster
+Home Theater Home Theatre
+Honeycomb
+Hook Claw
+Hoopskirt Crinoline
+Horizontal Bar High Bar
+Horse Cart Horse-Cart
+Hourglass
+Ipod
+Iron Smoothing Iron
+Jack-O-Lantern
+Jean Blue Jean Denim
+Jeep Landrover
+Jersey T-Shirt Tee Shirt
+Jigsaw Puzzle
+Jinrikisha Ricksha Rickshaw
+Joystick
+Kimono
+Knee Pad
+Knot
+Lab Coat Laboratory Coat
+Ladle
+Lampshade Lamp Shade
+Laptop Laptop Computer
+Lawn Mower Mower
+Lens Cap Lens Cover
+Letter Opener Paper Knife Paperknife
+Library
+Lifeboat
+Lighter Light Igniter Ignitor
+Limousine Limo
+Liner Ocean Liner
+Lipstick Lip Rouge
+Loafer
+Lotion
+Loudspeaker Speaker Speaker Unit Loudspeaker System Speaker System
+Loupe Jewelers Loupe
+Lumbermill Sawmill
+Magnetic Compass
+Mailbag Postbag
+Mailbox Letter Box
+Maillot
+Maillot Tank Suit
+Manhole Cover
+Maraca
+Marimba Xylophone
+Mask
+Matchstick
+Maypole
+Maze Labyrinth
+Measuring Cup
+Medicine Chest Medicine Cabinet
+Megalith Megalithic Structure
+Microphone Mike
+Microwave Microwave Oven
+Military Uniform
+Milk Can
+Minibus
+Miniskirt Mini
+Minivan
+Missile
+Mitten
+Mixing Bowl
+Mobile Home Manufactured Home
+Model T
+Modem
+Monastery
+Monitor
+Moped
+Mortar
+Mortarboard
+Mosque
+Mosquito Net
+Motor Scooter Scooter
+Mountain Bike All-Terrain Bike Off-Roader
+Mountain Tent
+Mouse Computer Mouse
+Mousetrap
+Moving Van
+Muzzle
+Nail
+Neck Brace
+Necklace
+Nipple
+Notebook Notebook Computer
+Obelisk
+Oboe Hautboy Hautbois
+Ocarina Sweet Potato
+Odometer Hodometer Mileometer Milometer
+Oil Filter
+Organ Pipe Organ
+Oscilloscope Scope Cathode-Ray Oscilloscope Cro
+Overskirt
+Oxcart
+Oxygen Mask
+Packet
+Paddle Boat Paddle
+Paddlewheel Paddle Wheel
+Padlock
+Paintbrush
+Pajama Pyjama Pjs Jammies
+Palace
+Panpipe Pandean Pipe Syrinx
+Paper Towel
+Parachute Chute
+Parallel Bars Bars
+Park Bench
+Parking Meter
+Passenger Car Coach Carriage
+Patio Terrace
+Pay-Phone Pay-Station
+Pedestal Plinth Footstall
+Pencil Box Pencil Case
+Pencil Sharpener
+Perfume Essence
+Petri Dish
+Photocopier
+Pick Plectrum Plectron
+Pickelhaube
+Picket Fence Paling
+Pickup Pickup Truck
+Pier
+Piggy Bank Penny Bank
+Pill Bottle
+Pillow
+Ping-Pong Ball
+Pinwheel
+Pirate Pirate Ship
+Pitcher Ewer
+Plane Carpenters Plane Woodworking Plane
+Planetarium
+Plastic Bag
+Plate Rack
+Plow Plough
+Plunger Plumbers Helper
+Polaroid Camera Polaroid Land Camera
+Pole
+Police Van Police Wagon Paddy Wagon Patrol Wagon Wagon Black Maria
+Poncho
+Pool Table Billiard Table Snooker Table
+Pop Bottle Soda Bottle
+Pot Flowerpot
+Potters Wheel
+Power Drill
+Prayer Rug Prayer Mat
+Printer
+Prison Prison House
+Projectile Missile
+Projector
+Puck Hockey Puck
+Punching Bag Punch Bag Punching Ball Punchball
+Purse
+Quill Quill Pen
+Quilt Comforter Comfort Puff
+Racer Race Car Racing Car
+Racket Racquet
+Radiator
+Radio Wireless
+Radio Telescope Radio Reflector
+Rain Barrel
+Recreational Vehicle Rv R.V.
+Reel
+Reflex Camera
+Refrigerator Icebox
+Remote Control Remote
+Restaurant Eating House Eating Place Eatery
+Revolver Six-Gun Six-Shooter
+Rifle
+Rocking Chair Rocker
+Rotisserie
+Rubber Eraser Rubber Pencil Eraser
+Rugby Ball
+Rule Ruler
+Running Shoe
+Safe
+Safety Pin
+Saltshaker Salt Shaker
+Sandal
+Sarong
+Sax Saxophone
+Scabbard
+Scale Weighing Machine
+School Bus
+Schooner
+Scoreboard
+Screen Crt Screen
+Screw
+Screwdriver
+Seat Belt Seatbelt
+Sewing Machine
+Shield Buckler
+Shoe Shop Shoe-Shop Shoe Store
+Shoji
+Shopping Basket
+Shopping Cart
+Shovel
+Shower Cap
+Shower Curtain
+Ski
+Ski Mask
+Sleeping Bag
+Slide Rule Slipstick
+Sliding Door
+Slot One-Armed Bandit
+Snorkel
+Snowmobile
+Snowplow Snowplough
+Soap Dispenser
+Soccer Ball
+Sock
+Solar Dish Solar Collector Solar Furnace
+Sombrero
+Soup Bowl
+Space Bar
+Space Heater
+Space Shuttle
+Spatula
+Speedboat
+Spider Web Spiders Web
+Spindle
+Sports Car Sport Car
+Spotlight Spot
+Stage
+Steam Locomotive
+Steel Arch Bridge
+Steel Drum
+Stethoscope
+Stole
+Stone Wall
+Stopwatch Stop Watch
+Stove
+Strainer
+Streetcar Tram Tramcar Trolley Trolley Car
+Stretcher
+Studio Couch Day Bed
+Stupa Tope
+Submarine Pigboat Sub U-Boat
+Suit Suit Of Clothes
+Sundial
+Sunglass
+Sunglasses Dark Glasses Shades
+Sunscreen Sunblock Sun Blocker
+Suspension Bridge
+Swab Swob Mop
+Sweatshirt
+Swimming Trunks Bathing Trunks
+Swing
+Switch Electric Switch Electrical Switch
+Syringe
+Table Lamp
+Tank Army Tank Armored Combat Vehicle Armoured Combat Vehicle
+Tape Player
+Teapot
+Teddy Teddy Bear
+Television Television System
+Tennis Ball
+Thatch Thatched Roof
+Theater Curtain Theatre Curtain
+Thimble
+Thresher Thrasher Threshing Machine
+Throne
+Tile Roof
+Toaster
+Tobacco Shop Tobacconist Shop Tobacconist
+Toilet Seat
+Torch
+Totem Pole
+Tow Truck Tow Car Wrecker
+Toyshop
+Tractor
+Trailer Truck Tractor Trailer Trucking Rig Rig Articulated Lorry Semi
+Tray
+Trench Coat
+Tricycle Trike Velocipede
+Trimaran
+Tripod
+Triumphal Arch
+Trolleybus Trolley Coach Trackless Trolley
+Trombone
+Tub Vat
+Turnstile
+Typewriter Keyboard
+Umbrella
+Unicycle Monocycle
+Upright Upright Piano
+Vacuum Vacuum Cleaner
+Vase
+Vault
+Velvet
+Vending Machine
+Vestment
+Viaduct
+Violin Fiddle
+Volleyball
+Waffle Iron
+Wall Clock
+Wallet Billfold Notecase Pocketbook
+Wardrobe Closet Press
+Warplane Military Plane
+Washbasin Handbasin Washbowl Lavabo Wash-Hand Basin
+Washer Automatic Washer Washing Machine
+Water Bottle
+Water Jug
+Water Tower
+Whiskey Jug
+Whistle
+Wig
+Window Screen
+Window Shade
+Windsor Tie
+Wine Bottle
+Wing
+Wok
+Wooden Spoon
+Wool Woolen Woollen
+Worm Fence Snake Fence Snake-Rail Fence Virginia Fence
+Wreck
+Yawl
+Yurt
+Web Site Website Internet Site Site
+Comic Book
+Crossword Puzzle Crossword
+Street Sign
+Traffic Light Traffic Signal Stoplight
+Book Jacket Dust Cover Dust Jacket Dust Wrapper
+Menu
+Plate
+Guacamole
+Consomme
+Hot Pot Hotpot
+Trifle
+Ice Cream Icecream
+Ice Lolly Lolly Lollipop Popsicle
+French Loaf
+Bagel Beigel
+Pretzel
+Cheeseburger
+Hotdog Hot Dog Red Hot
+Mashed Potato
+Head Cabbage
+Broccoli
+Cauliflower
+Zucchini Courgette
+Spaghetti Squash
+Acorn Squash
+Butternut Squash
+Cucumber Cuke
+Artichoke Globe Artichoke
+Bell Pepper
+Cardoon
+Mushroom
+Granny Smith
+Strawberry
+Orange
+Lemon
+Fig
+Pineapple Ananas
+Banana
+Jackfruit Jak Jack
+Custard Apple
+Pomegranate
+Hay
+Carbonara
+Chocolate Sauce Chocolate Syrup
+Dough
+Meat Loaf Meatloaf
+Pizza Pizza Pie
+Potpie
+Burrito
+Red Wine
+Espresso
+Cup
+Eggnog
+Alp
+Bubble
+Cliff Drop Drop-Off
+Coral Reef
+Geyser
+Lakeside Lakeshore
+Promontory Headland Head Foreland
+Sandbar Sand Bar
+Seashore Coast Seacoast Sea-Coast
+Valley Vale
+Volcano
+Ballplayer Baseball Player
+Groom Bridegroom
+Scuba Diver
+Rapeseed
+Daisy
+Yellow Ladys Slipper Yellow Lady-Slipper Cypripedium Calceolus Cypripedium Parviflorum
+Corn
+Acorn
+Hip Rose Hip Rosehip
+Buckeye Horse Chestnut Conker
+Coral Fungus
+Agaric
+Gyromitra
+Stinkhorn Carrion Fungus
+Earthstar
+Hen-Of-The-Woods Hen Of The Woods Polyporus Frondosus Grifola Frondosa
+Bolete
+Ear Spike Capitulum
+Toilet Tissue Toilet Paper Bathroom Tissue
diff --git a/tidl_demos/tidl_build.sh b/tidl_demos/tidl_build.sh
new file mode 100755
index 000000000..751e0ecfe
--- /dev/null
+++ b/tidl_demos/tidl_build.sh
@@ -0,0 +1,9 @@
+SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" >/dev/null 2>&1 && pwd )"
+ORT_ROOT=$SCRIPT_DIR/..
+
+cd $ORT_ROOT
+
+./build.sh --config RelWithDebInfo --build_shared_lib --parallel --skip_tests --skip_onnx_tests --use_tidl --use_dnnl
+#./build.sh --config Debug --build_shared_lib --parallel --skip_tests --skip_onnx_tests --use_tidl --use_dnnl
+
+cd -
diff --git a/tidl_demos/tools/pics.py b/tidl_demos/tools/pics.py
new file mode 100644
index 000000000..f2361dfa4
--- /dev/null
+++ b/tidl_demos/tools/pics.py
@@ -0,0 +1,39 @@
+from PIL import Image
+import numpy as np
+
+def preprocess(img_path):
+    img = Image.open(img_path)
+    img = img.resize((224, 224), Image.BILINEAR)
+    img_data = np.array(img, dtype=np.float32)
+    img_data = np.transpose(img_data, [2, 0, 1])
+    img_data = np.expand_dims(img_data, 0)
+    img_data = img_data[:,0:3,:,:] # Input is RGBA - so strip off A
+    return img_data
+
+mushroom = preprocess('cheetah.png')
+
+print "std::vector<float> mushroom = {"
+for a in mushroom:
+    for b in a:
+        for c in b:
+            for d in c:
+                print "    " + str(d) + ","
+print "};"
+
+# x=0
+# y=0
+
+# im = Image.open('mushroom.png') # Can be many different formats.
+# pix = im.load()
+# print im.size  # Get the width and hight of the image for iterating over
+
+# pix[x,y] = value  # Set the RGBA Value of the image (tuple)
+
+# print "std::vector<float> mushroom = {"
+# for x in range(im.size[0]):
+    # for y in range(im.size[1]):
+        # print '    ' + ','.join(map(str,pix[y,x][0:3])) + ','  # Get the RGBA Value of the a pixel of an image
+        # pix[x,y] = (0,0,0)
+        # pass
+# print "};"
+
diff --git a/tool.cmake b/tool.cmake
new file mode 100644
index 000000000..fcdf8a275
--- /dev/null
+++ b/tool.cmake
@@ -0,0 +1,9 @@
+SET(CMAKE_SYSTEM_NAME Linux)
+SET(CMAKE_SYSTEM_VERSION 1)
+SET(CMAKE_C_COMPILER aarch64-none-linux-gnu-gcc)
+SET(CMAKE_CXX_COMPILER aarch64-none-linux-gnu-g++)
+SET(CMAKE_FIND_ROOT_PATH_MODE_PROGRAM NEVER)
+SET(CMAKE_FIND_ROOT_PATH_MODE_LIBRARY ONLY)
+SET(CMAKE_FIND_ROOT_PATH_MODE_INCLUDE ONLY)
+SET(CMAKE_FIND_ROOT_PATH_MODE_PACKAGE ONLY)
+SET(CMAKE_FIND_ROOT_PATH /home/a0393754/work/psdkra/targetfs)
diff --git a/tools/ci_build/build.py b/tools/ci_build/build.py
index 45e1ddef7..564d17b5c 100644
--- a/tools/ci_build/build.py
+++ b/tools/ci_build/build.py
@@ -323,6 +323,8 @@ def parse_arguments():
         choices=['none', 'stl', 'arena', 'all'], help="Use mimalloc.")
     parser.add_argument(
         "--use_dnnl", action='store_true', help="Build with DNNL.")
+    parser.add_argument(
+        "--use_tidl", action='store_true', help="Build with TIDL.")
     parser.add_argument(
         "--dnnl_gpu_runtime", action='store', default='', type=str.lower,
         help="e.g. --dnnl_gpu_runtime ocl")
@@ -652,6 +654,7 @@ def generate_build_tree(cmake_path, source_dir, build_dir, cuda_home, cudnn_home
         "-Donnxruntime_BUILD_JAVA=" + ("ON" if args.build_java else "OFF"),
         "-Donnxruntime_BUILD_NODEJS=" + ("ON" if args.build_nodejs else "OFF"),
         "-Donnxruntime_BUILD_SHARED_LIB=" + ("ON" if args.build_shared_lib else "OFF"),
+        "-Donnxruntime_USE_TIDL=" + ("ON" if args.use_tidl else "OFF"),
         "-Donnxruntime_USE_DNNL=" + ("ON" if args.use_dnnl else "OFF"),
         "-Donnxruntime_DNNL_GPU_RUNTIME=" + (args.dnnl_gpu_runtime if args.use_dnnl else ""),
         "-Donnxruntime_DNNL_OPENCL_ROOT=" + (args.dnnl_opencl_root if args.use_dnnl else ""),
@@ -1461,7 +1464,7 @@ def run_nodejs_tests(nodejs_binding_dir):
 
 def build_python_wheel(
         source_dir, build_dir, configs, use_cuda, use_dnnl,
-        use_tensorrt, use_openvino, use_nuphar, use_vitisai, use_acl, use_armnn, use_dml,
+        use_tensorrt, use_openvino, use_nuphar, use_tidl, use_vitisai, use_acl, use_armnn, use_dml,
         wheel_name_suffix, enable_training, nightly_build=False, featurizers_build=False, use_ninja=False):
     for config in configs:
         cwd = get_config_build_dir(build_dir, config)
@@ -1500,6 +1503,8 @@ def build_python_wheel(
             args.append('--use_openvino')
         elif use_dnnl:
             args.append('--use_dnnl')
+        elif use_tidl:
+            args.append('--use_tidl')
         elif use_nuphar:
             args.append('--use_nuphar')
         elif use_vitisai:
@@ -1961,6 +1966,7 @@ def main():
                 args.use_tensorrt,
                 args.use_openvino,
                 args.use_nuphar,
+                args.use_tidl, 
                 args.use_vitisai,
                 args.use_acl,
                 args.use_armnn,
-- 
2.17.1

